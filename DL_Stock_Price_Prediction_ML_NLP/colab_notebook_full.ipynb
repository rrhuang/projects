{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "794989b18e6f4f07afb5ca4c67f5f77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a201ce5132da4a879d276ba7b24b91b2",
              "IPY_MODEL_efa523229fa641ddb74486fcee47ca6f",
              "IPY_MODEL_96d011524c4c4ad0b171e4479e196dfb"
            ],
            "layout": "IPY_MODEL_962d0ba416d04f31bd5de90f531d6035"
          }
        },
        "a201ce5132da4a879d276ba7b24b91b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aa3b419e05a42d482e9c87d6163049f",
            "placeholder": "​",
            "style": "IPY_MODEL_3e42545a71b947e1905617d4054947df",
            "value": "model.safetensors: 100%"
          }
        },
        "efa523229fa641ddb74486fcee47ca6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41f09db69385438db730e13a3472e565",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ba79f10b05b4eec8bb95d54a0257edb",
            "value": 267954768
          }
        },
        "96d011524c4c4ad0b171e4479e196dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf7bca4832ca40958f42bcaede6a054d",
            "placeholder": "​",
            "style": "IPY_MODEL_789d85aaad164e19ad5bfb592aeca3ba",
            "value": " 268M/268M [00:02&lt;00:00, 132MB/s]"
          }
        },
        "962d0ba416d04f31bd5de90f531d6035": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aa3b419e05a42d482e9c87d6163049f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e42545a71b947e1905617d4054947df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41f09db69385438db730e13a3472e565": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ba79f10b05b4eec8bb95d54a0257edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf7bca4832ca40958f42bcaede6a054d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "789d85aaad164e19ad5bfb592aeca3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "fQVcq7u4X0_o"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices = pd.read_csv('/content/h1_2015_MSFT_prices_tech.csv')"
      ],
      "metadata": {
        "id": "8Qty1FghX-Nz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices = prices.iloc[4:]\n",
        "\n",
        "prices.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ys-JT6BkYLa3",
        "outputId": "256761e9-f7d9-4c57-9460-d2d3e5e0d76d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  TIME      PRICE     SIZE   returns  return_label ticker  \\\n",
              "4  2015-01-02 09:00:00  46.651600    43628 -0.058400          -1.0   MSFT   \n",
              "5  2015-01-02 10:00:00  47.014152  5102614  0.362552           1.0   MSFT   \n",
              "6  2015-01-02 11:00:00  47.064821  4597166  0.050669           1.0   MSFT   \n",
              "7  2015-01-02 12:00:00  46.701520  2944126 -0.363301          -1.0   MSFT   \n",
              "8  2015-01-02 13:00:00  46.763572  1970859  0.062053           1.0   MSFT   \n",
              "\n",
              "     SP-EMA5     RDP-5  RDP-10  RDP-15  RDP-20      MACD      OBV  Volatility  \\\n",
              "4  46.624651       NaN     NaN     NaN     NaN  0.034299   -43628         NaN   \n",
              "5  46.754485  1.105703     NaN     NaN     NaN  0.069454  5058986         NaN   \n",
              "6  46.857930       NaN     NaN     NaN     NaN  0.098662  9656152         NaN   \n",
              "7  46.805793       NaN     NaN     NaN     NaN  0.087120  6712026         NaN   \n",
              "8  46.791720  0.114691     NaN     NaN     NaN  0.082440  8682885         NaN   \n",
              "\n",
              "      RPD+5  \n",
              "4 -0.025580  \n",
              "5 -0.665590  \n",
              "6 -0.761642  \n",
              "7 -0.087627  \n",
              "8  0.108552  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33409f8b-d511-44bb-aa81-c2d4ed744441\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TIME</th>\n",
              "      <th>PRICE</th>\n",
              "      <th>SIZE</th>\n",
              "      <th>returns</th>\n",
              "      <th>return_label</th>\n",
              "      <th>ticker</th>\n",
              "      <th>SP-EMA5</th>\n",
              "      <th>RDP-5</th>\n",
              "      <th>RDP-10</th>\n",
              "      <th>RDP-15</th>\n",
              "      <th>RDP-20</th>\n",
              "      <th>MACD</th>\n",
              "      <th>OBV</th>\n",
              "      <th>Volatility</th>\n",
              "      <th>RPD+5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015-01-02 09:00:00</td>\n",
              "      <td>46.651600</td>\n",
              "      <td>43628</td>\n",
              "      <td>-0.058400</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>46.624651</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.034299</td>\n",
              "      <td>-43628</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.025580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2015-01-02 10:00:00</td>\n",
              "      <td>47.014152</td>\n",
              "      <td>5102614</td>\n",
              "      <td>0.362552</td>\n",
              "      <td>1.0</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>46.754485</td>\n",
              "      <td>1.105703</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.069454</td>\n",
              "      <td>5058986</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.665590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2015-01-02 11:00:00</td>\n",
              "      <td>47.064821</td>\n",
              "      <td>4597166</td>\n",
              "      <td>0.050669</td>\n",
              "      <td>1.0</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>46.857930</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.098662</td>\n",
              "      <td>9656152</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.761642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2015-01-02 12:00:00</td>\n",
              "      <td>46.701520</td>\n",
              "      <td>2944126</td>\n",
              "      <td>-0.363301</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>46.805793</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.087120</td>\n",
              "      <td>6712026</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.087627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2015-01-02 13:00:00</td>\n",
              "      <td>46.763572</td>\n",
              "      <td>1970859</td>\n",
              "      <td>0.062053</td>\n",
              "      <td>1.0</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>46.791720</td>\n",
              "      <td>0.114691</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.082440</td>\n",
              "      <td>8682885</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.108552</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33409f8b-d511-44bb-aa81-c2d4ed744441')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-33409f8b-d511-44bb-aa81-c2d4ed744441 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-33409f8b-d511-44bb-aa81-c2d4ed744441');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-227fdf62-39ff-4fcb-b07a-3793b51aef8e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-227fdf62-39ff-4fcb-b07a-3793b51aef8e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-227fdf62-39ff-4fcb-b07a-3793b51aef8e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "prices",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prices['datetime'] = pd.to_datetime(prices['TIME'], utc=True)\n",
        "prices['datetime'] = prices['datetime'].dt.tz_convert('America/New_York')"
      ],
      "metadata": {
        "id": "hWdQYH_bdf3z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = pd.read_csv('/content/h1_2015_msft_aggtweet.csv')"
      ],
      "metadata": {
        "id": "fMY3oiU1YfNv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BYSSAHrDjuY5",
        "outputId": "28bc660e-0eee-45f0-de4f-52a9124c9602"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                   datetime  \\\n",
              "0              0  2015-01-01 06:00:00-05:00   \n",
              "1              1  2015-01-01 07:00:00-05:00   \n",
              "2              2  2015-01-01 08:00:00-05:00   \n",
              "3              3  2015-01-01 09:00:00-05:00   \n",
              "4              4  2015-01-01 10:00:00-05:00   \n",
              "...          ...                        ...   \n",
              "3171        3171  2015-06-01 16:00:00-04:00   \n",
              "3172        3172  2015-06-01 17:00:00-04:00   \n",
              "3173        3173  2015-06-01 18:00:00-04:00   \n",
              "3174        3174  2015-06-01 19:00:00-04:00   \n",
              "3175        3175  2015-06-01 20:00:00-04:00   \n",
              "\n",
              "                                       aggregated_tweet  \n",
              "0     Stock Focus: Microsoft (NASDAQ MSFT) Would Go ...  \n",
              "1     MY BOOK $SNE short Close $20.47 52wk High $22....  \n",
              "2     Roadmap to become an #options expert http://bi...  \n",
              "3     2015 Tech Forecast: Clouds, Wearables, Cyberse...  \n",
              "4     Microsoft Corporation Is About to Abandon Inte...  \n",
              "...                                                 ...  \n",
              "3171  Our Pick On $PPCH Gained As Much As 400% For S...  \n",
              "3172  $MSFT Microsoft Corporation (NASDAQ:MSFT) Wind...  \n",
              "3173  #Microsoft to launch #Windows10 on July 29 & #...  \n",
              "3174  @rrabg5 Your tweet about $MSFT had a sentiment...  \n",
              "3175  The new Tomb Raider trailer is all about disco...  \n",
              "\n",
              "[3176 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c168356-beff-4ff5-86e6-0aadff2e0a28\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>datetime</th>\n",
              "      <th>aggregated_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2015-01-01 06:00:00-05:00</td>\n",
              "      <td>Stock Focus: Microsoft (NASDAQ MSFT) Would Go ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2015-01-01 07:00:00-05:00</td>\n",
              "      <td>MY BOOK $SNE short Close $20.47 52wk High $22....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2015-01-01 08:00:00-05:00</td>\n",
              "      <td>Roadmap to become an #options expert http://bi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2015-01-01 09:00:00-05:00</td>\n",
              "      <td>2015 Tech Forecast: Clouds, Wearables, Cyberse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2015-01-01 10:00:00-05:00</td>\n",
              "      <td>Microsoft Corporation Is About to Abandon Inte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3171</th>\n",
              "      <td>3171</td>\n",
              "      <td>2015-06-01 16:00:00-04:00</td>\n",
              "      <td>Our Pick On $PPCH Gained As Much As 400% For S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3172</th>\n",
              "      <td>3172</td>\n",
              "      <td>2015-06-01 17:00:00-04:00</td>\n",
              "      <td>$MSFT Microsoft Corporation (NASDAQ:MSFT) Wind...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3173</th>\n",
              "      <td>3173</td>\n",
              "      <td>2015-06-01 18:00:00-04:00</td>\n",
              "      <td>#Microsoft to launch #Windows10 on July 29 &amp; #...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3174</th>\n",
              "      <td>3174</td>\n",
              "      <td>2015-06-01 19:00:00-04:00</td>\n",
              "      <td>@rrabg5 Your tweet about $MSFT had a sentiment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3175</th>\n",
              "      <td>3175</td>\n",
              "      <td>2015-06-01 20:00:00-04:00</td>\n",
              "      <td>The new Tomb Raider trailer is all about disco...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3176 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c168356-beff-4ff5-86e6-0aadff2e0a28')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8c168356-beff-4ff5-86e6-0aadff2e0a28 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8c168356-beff-4ff5-86e6-0aadff2e0a28');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-745fd9ad-0315-46e9-9b39-6f9d9a2e7a81\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-745fd9ad-0315-46e9-9b39-6f9d9a2e7a81')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-745fd9ad-0315-46e9-9b39-6f9d9a2e7a81 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_8300a939-21f4-4421-ac90-2230249ca5b4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('tweets')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8300a939-21f4-4421-ac90-2230249ca5b4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('tweets');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tweets",
              "summary": "{\n  \"name\": \"tweets\",\n  \"rows\": 3176,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 916,\n        \"min\": 0,\n        \"max\": 3175,\n        \"num_unique_values\": 3176,\n        \"samples\": [\n          2270,\n          442,\n          2886\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"datetime\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 3176,\n        \"samples\": [\n          \"2015-04-18 22:00:00-04:00\",\n          \"2015-01-23 01:00:00-05:00\",\n          \"2015-05-19 08:00:00-04:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"aggregated_tweet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3116,\n        \"samples\": [\n          \"Google's share of U.S. search dipped below 75% for the first time http://ibdn.uz/In4mp $GOOGL $YHOO $MSFT Our Alerts on $IJJP $QLTS $TUNG & $ETBI Gained 160% last Week! Get Our Next Pick: http://tinyurl.com/k7vw6pt $XLF $F $FN $MSFT Predict stock prices movement and earn money  $AAPL $BAC $FXCM $CSCO  $AXP $BBRY $GE $QQQ $PLUG $T $MSFT $INTC $ZIOP http://marketjolt.com $MSFT - Statement of Changes in Beneficial Ownership (4) http://ih.advfn.com/p.php?pid=nmona&article=65327975&xref=newsalerttweet&adw=1126416\\u2026 $MSFT - Statement of Changes in Beneficial Ownership (4) http://uk.advfn.com/news/EDGAR/2015/article/65327975?xref=newsalerttweet&adw=1126416\\u2026 $MSFT - Statement of Changes in Beneficial Ownership (4) http://uk.advfn.com/news/EDGAR/2015/article/65327975?xref=newsalerttweet&adw=1126416\\u2026 $MSFT chart http://stks.co/h1fuN doesn't look like it will continue to go up tomorrow. RT @TheStreetTV $MSFT Microsoft Gains After Announcing Windows 10 Software Deal $MSFT - Statement of Changes in Beneficial Ownership (4) http://ih.advfn.com/p.php?pid=nmona&article=65327990&xref=newsalerttweet&adw=1126416\\u2026 $MSFT - Statement of Changes in Beneficial Ownership (4) http://ih.advfn.com/p.php?pid=nmona&article=65327994&xref=newsalerttweet&adw=1126416\\u2026 $MSFT - Statement of Changes in Beneficial Ownership (4) http://uk.advfn.com/news/EDGAR/2015/article/65327990?xref=newsalerttweet&adw=1126416\\u2026 $MSFT - Statement of Changes in Beneficial Ownership (4) http://uk.advfn.com/news/EDGAR/2015/article/65327994?xref=newsalerttweet&adw=1126416\\u2026 $MSFT - Statement of Changes in Beneficial Ownership (4) http://uk.advfn.com/news/EDGAR/2015/article/65327990?xref=newsalerttweet&adw=1126416\\u2026 $MSFT - Statement of Changes in Beneficial Ownership (4) http://uk.advfn.com/news/EDGAR/2015/article/65327994?xref=newsalerttweet&adw=1126416\\u2026 $MSFT - Statement of Changes in Beneficial Ownership (4) http://ih.advfn.com/p.php?pid=nmona&article=65328011&xref=newsalerttweet&adw=1126416\\u2026 $MSFT - Statement of Changes in Beneficial Ownership (4) http://ih.advfn.com/p.php?pid=nmona&article=65328012&xref=newsalerttweet&adw=1126416\\u2026 $MSFT - Statement of Changes in Beneficial Ownership (4) http://ih.advfn.com/p.php?pid=nmona&article=65328014&xref=newsalerttweet&adw=1126416\\u2026 $MSFT - Statement of Changes in Beneficial Ownership (4) http://uk.advfn.com/news/EDGAR/2015/article/65328011?xref=newsalerttweet&adw=1126416\\u2026 $MSFT New SEC Document(s) for Microsoft CorporationFrom our Stock News Alerts App Better Bad News Buy: IBM or Microsoft Corporation? $MSFT http://bit.ly/1yxbiuk $MSFT - Statement of Changes in Beneficial Ownership (4) http://ih.advfn.com/p.php?pid=nmona&article=65328155&xref=newsalerttweet&adw=1126416\\u2026 $MSFT - Statement of Changes in Beneficial Ownership (4) Prior RT hopefully means $MSFT is pricing/giving away #Win10 based on SoC TDP instead of screen size. Good. $MSFT - Statement of Changes in Beneficial Ownership (4)\",\n          \"@kevinhuanghk Your tweet about $MSFT had a sentiment of 1 and was featured on Market Parse. http://marketparse.com Good morning, trading friends. Make it a great day! $MSFT $SPY $F $AAPL $HYG $IMAX http://bit.ly/1Lhpqkk $MSFT $SPY $F $AAPL $HYG $IMAX $RIG $YHOO $USO $HUN http://bit.ly/1Lhpqkk This Man Turned $100 Into Over $10,000 In Just 30 Days! Proof Here http://PennyStockSecret.com .. $NTEK $MSFT $MWIP\",\n          \"As former PM Head of $MSFT .NET Services (\\\"Hailstorm\\\"), great to see my old boss @billgates still carrying the torch! #Microsoft : Patent Issued for Hardware Management Communication Protocol http://4-traders.com/MICROSOFT-CORPORATION-4835/news/Microsoft--Patent-Issued-for-Hardware-Management-Communication-Protocol-19778466/\\u2026 $MSFT @microsoft's @BillGates says he feels \\\"stupid\\\" for not knowing any other language. https://inside.com/billgates/u52nd/Microsoft-s-Bill-Gates-says-he-feels-stupid-for?utm_source=twitter.com&utm_medium=twitterhandle&utm_campaign=@getinmicrosoft\\u2026 $msft $qcom at 2011 price. $msft down 20% in 2 weeks or so. this market inspires NO confidence. 2015 can be down 20% easy on $spx lol Bill Gates On Microsoft: 'More Progress Than Ever In Next 30 Years' $MSFT http://benzinga.com/news/15/01/5188109/bill-gates-on-microsoft-more-progress-than-ever-in-next-30-years\\u2026 @BillGates If you don't consistently find trades like $HGSH +47%, then http://BiloSelhi.com  $XOM $MSFT $INTC $AMZN $GOOGL $GI $MSFT - Microsoft HoloLens Gets Face Wearables Right\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['datetime'] = pd.to_datetime(tweets['datetime'], utc=True)\n",
        "\n",
        "tweets['datetime'] = tweets['datetime'].dt.tz_convert('America/New_York')"
      ],
      "metadata": {
        "id": "0G9z6hkwa_zE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = pd.merge(prices, tweets, left_on='datetime', right_on='datetime')"
      ],
      "metadata": {
        "id": "D9h3UIDSdmn4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "na_returns = merged[merged['returns'].isna()]\n",
        "na_returns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "vNiPxRA7xg-w",
        "outputId": "78f39634-b7b3-4752-96e2-9459727b9687"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     TIME  PRICE  SIZE  returns  return_label ticker  \\\n",
              "12    2015-01-02 21:00:00    NaN     0      NaN           NaN   MSFT   \n",
              "13    2015-01-02 22:00:00    NaN     0      NaN           NaN   MSFT   \n",
              "14    2015-01-02 23:00:00    NaN     0      NaN           NaN   MSFT   \n",
              "15    2015-01-03 00:00:00    NaN     0      NaN           NaN   MSFT   \n",
              "16    2015-01-03 01:00:00    NaN     0      NaN           NaN   MSFT   \n",
              "...                   ...    ...   ...      ...           ...    ...   \n",
              "3131  2015-06-01 01:00:00    NaN     0      NaN           NaN   MSFT   \n",
              "3132  2015-06-01 02:00:00    NaN     0      NaN           NaN   MSFT   \n",
              "3133  2015-06-01 03:00:00    NaN     0      NaN           NaN   MSFT   \n",
              "3134  2015-06-01 04:00:00    NaN     0      NaN           NaN   MSFT   \n",
              "3135  2015-06-01 05:00:00  47.07   110      NaN           NaN   MSFT   \n",
              "\n",
              "        SP-EMA5  RDP-5  RDP-10  RDP-15  RDP-20      MACD      OBV  Volatility  \\\n",
              "12    46.743603    NaN     NaN     NaN     NaN  0.044515  9323502         NaN   \n",
              "13    46.743603    NaN     NaN     NaN     NaN  0.044515  9323502         NaN   \n",
              "14    46.743603    NaN     NaN     NaN     NaN  0.044515  9323502         NaN   \n",
              "15    46.743603    NaN     NaN     NaN     NaN  0.044515  9323502         NaN   \n",
              "16    46.743603    NaN     NaN     NaN     NaN  0.044515  9323502         NaN   \n",
              "...         ...    ...     ...     ...     ...       ...      ...         ...   \n",
              "3131  46.952895    NaN     NaN     NaN     NaN -0.109312  3903465         NaN   \n",
              "3132  46.952895    NaN     NaN     NaN     NaN -0.109312  3903465         NaN   \n",
              "3133  46.952895    NaN     NaN     NaN     NaN -0.109312  3903465         NaN   \n",
              "3134  46.952895    NaN     NaN     NaN     NaN -0.109312  3903465         NaN   \n",
              "3135  47.070000    NaN     NaN     NaN     NaN -0.001652  3903465         NaN   \n",
              "\n",
              "         RPD+5                  datetime  Unnamed: 0  \\\n",
              "12         NaN 2015-01-02 16:00:00-05:00          33   \n",
              "13         NaN 2015-01-02 17:00:00-05:00          34   \n",
              "14         NaN 2015-01-02 18:00:00-05:00          35   \n",
              "15         NaN 2015-01-02 19:00:00-05:00          36   \n",
              "16         NaN 2015-01-02 20:00:00-05:00          37   \n",
              "...        ...                       ...         ...   \n",
              "3131       NaN 2015-05-31 21:00:00-04:00        3152   \n",
              "3132       NaN 2015-05-31 22:00:00-04:00        3153   \n",
              "3133       NaN 2015-05-31 23:00:00-04:00        3154   \n",
              "3134       NaN 2015-06-01 00:00:00-04:00        3155   \n",
              "3135 -0.095695 2015-06-01 01:00:00-04:00        3156   \n",
              "\n",
              "                                       aggregated_tweet  \n",
              "12    2015 Outlook: Winners: $MSFT $FB $TWTR $BAC $J...  \n",
              "13    What kind of trades are good during expiration...  \n",
              "14    Microsoft's Windows 8 Suffers Embarrassing Set...  \n",
              "15    S&P100 #Stocks Performance $APA $SPG $LLY $EXC...  \n",
              "16    $MSFT Microsoft Health Breaks Into the Wearabl...  \n",
              "...                                                 ...  \n",
              "3131  Our Penny Stock Pick On $THCZ Is Up 570.77% Fo...  \n",
              "3132  .@Windows 8 or 8.1 user? Reserve your copy of ...  \n",
              "3133  $MSFT pushing out a notice to current Windows ...  \n",
              "3134  $MSFT Earnings are Deteriorating. #CML_MSFT #C...  \n",
              "3135  Nate made $799  on $MSFT -Check it out! http:/...  \n",
              "\n",
              "[1823 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-602c6931-f106-41ce-8c0e-11d9feca0976\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TIME</th>\n",
              "      <th>PRICE</th>\n",
              "      <th>SIZE</th>\n",
              "      <th>returns</th>\n",
              "      <th>return_label</th>\n",
              "      <th>ticker</th>\n",
              "      <th>SP-EMA5</th>\n",
              "      <th>RDP-5</th>\n",
              "      <th>RDP-10</th>\n",
              "      <th>RDP-15</th>\n",
              "      <th>RDP-20</th>\n",
              "      <th>MACD</th>\n",
              "      <th>OBV</th>\n",
              "      <th>Volatility</th>\n",
              "      <th>RPD+5</th>\n",
              "      <th>datetime</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>aggregated_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2015-01-02 21:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>46.743603</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.044515</td>\n",
              "      <td>9323502</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-01-02 16:00:00-05:00</td>\n",
              "      <td>33</td>\n",
              "      <td>2015 Outlook: Winners: $MSFT $FB $TWTR $BAC $J...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2015-01-02 22:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>46.743603</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.044515</td>\n",
              "      <td>9323502</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-01-02 17:00:00-05:00</td>\n",
              "      <td>34</td>\n",
              "      <td>What kind of trades are good during expiration...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2015-01-02 23:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>46.743603</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.044515</td>\n",
              "      <td>9323502</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-01-02 18:00:00-05:00</td>\n",
              "      <td>35</td>\n",
              "      <td>Microsoft's Windows 8 Suffers Embarrassing Set...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2015-01-03 00:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>46.743603</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.044515</td>\n",
              "      <td>9323502</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-01-02 19:00:00-05:00</td>\n",
              "      <td>36</td>\n",
              "      <td>S&amp;P100 #Stocks Performance $APA $SPG $LLY $EXC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2015-01-03 01:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>46.743603</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.044515</td>\n",
              "      <td>9323502</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-01-02 20:00:00-05:00</td>\n",
              "      <td>37</td>\n",
              "      <td>$MSFT Microsoft Health Breaks Into the Wearabl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3131</th>\n",
              "      <td>2015-06-01 01:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>46.952895</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.109312</td>\n",
              "      <td>3903465</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-05-31 21:00:00-04:00</td>\n",
              "      <td>3152</td>\n",
              "      <td>Our Penny Stock Pick On $THCZ Is Up 570.77% Fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3132</th>\n",
              "      <td>2015-06-01 02:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>46.952895</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.109312</td>\n",
              "      <td>3903465</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-05-31 22:00:00-04:00</td>\n",
              "      <td>3153</td>\n",
              "      <td>.@Windows 8 or 8.1 user? Reserve your copy of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3133</th>\n",
              "      <td>2015-06-01 03:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>46.952895</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.109312</td>\n",
              "      <td>3903465</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-05-31 23:00:00-04:00</td>\n",
              "      <td>3154</td>\n",
              "      <td>$MSFT pushing out a notice to current Windows ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3134</th>\n",
              "      <td>2015-06-01 04:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>46.952895</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.109312</td>\n",
              "      <td>3903465</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-06-01 00:00:00-04:00</td>\n",
              "      <td>3155</td>\n",
              "      <td>$MSFT Earnings are Deteriorating. #CML_MSFT #C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3135</th>\n",
              "      <td>2015-06-01 05:00:00</td>\n",
              "      <td>47.07</td>\n",
              "      <td>110</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>47.070000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.001652</td>\n",
              "      <td>3903465</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.095695</td>\n",
              "      <td>2015-06-01 01:00:00-04:00</td>\n",
              "      <td>3156</td>\n",
              "      <td>Nate made $799  on $MSFT -Check it out! http:/...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1823 rows × 18 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-602c6931-f106-41ce-8c0e-11d9feca0976')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-602c6931-f106-41ce-8c0e-11d9feca0976 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-602c6931-f106-41ce-8c0e-11d9feca0976');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ce9a4936-1879-45e8-8aad-9376ba2a354f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ce9a4936-1879-45e8-8aad-9376ba2a354f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ce9a4936-1879-45e8-8aad-9376ba2a354f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_4bbfb2eb-0465-4a8c-812e-f428877123a6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('na_returns')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4bbfb2eb-0465-4a8c-812e-f428877123a6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('na_returns');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "na_returns",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tech = prices[['returns','SP-EMA5','OBV','MACD']]"
      ],
      "metadata": {
        "id": "JBKT1sd2avi-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = merged.dropna(subset=['returns', 'PRICE'])\n"
      ],
      "metadata": {
        "id": "qfBg6n5QfaWO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_features = ['PRICE', 'SIZE']\n",
        "tech_features = ['returns','SP-EMA5','OBV','MACD']\n",
        "X_prices = merged[price_features].values\n",
        "X_tech = merged[tech_features].values\n",
        "\n",
        "y = merged['return_label'].shift(-1).dropna()\n",
        "X_prices = X_prices[:-1]\n",
        "X_tech = X_tech[:-1]"
      ],
      "metadata": {
        "id": "FceOALdxebih"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Concatenate, Conv1D, GlobalMaxPooling1D, Dropout, Attention, Reshape\n",
        "from transformers import TFDistilBertModel, DistilBertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
      ],
      "metadata": {
        "id": "6FkwKw1GeyCv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(y)\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "y_encoded = onehot_encoder.fit_transform(integer_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlg3oQObeuwj",
        "outputId": "7da2c1d6-fdd3-44d2-b0a1-b11687964453"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "max_seq_length = 128  # Reduce the sequence length to limit memory usage\n",
        "tweet_texts = merged['aggregated_tweet'].values  # Assuming 'tweets' column has the aggregated tweets\n",
        "tweet_encodings = tokenizer(list(tweet_texts[:-1]), truncation=True, padding=True, max_length=max_seq_length, return_tensors=\"tf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKx9yeZye70b",
        "outputId": "00e6dceb-bd3e-4cce-d777-9620ce2f3fbe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_encodings.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJeLeLXqltSj",
        "outputId": "2ba26dff-ab3e-4270-d516-a8f345b6af20"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tweet_encodings['input_ids']\n",
        "attention_mask = tweet_encodings['attention_mask']\n",
        "#token_type_ids = tweet_encodings['token_type_ids']"
      ],
      "metadata": {
        "id": "s_ZFHWO8itOV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_prices_train, X_prices_test, X_tech_train, X_tech_test, y_train, y_test = train_test_split(\n",
        "    X_prices, X_tech, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_prices_train, X_prices_val, X_tech_train, X_tech_val, y_train, y_val = train_test_split(\n",
        "    X_prices_train, X_tech_train, y_train, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "Gx_X-QNZfIwW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.37.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "7rTGdiOwBNbe",
        "outputId": "93e415c0-d5ea-41fc-966f-954ee012ce40"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.37.2\n",
            "  Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.37.2)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2024.6.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "Successfully installed tokenizers-0.15.2 transformers-4.37.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "5c6781d9ec904d2a9e56d125cf74fb8f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "794989b18e6f4f07afb5ca4c67f5f77b",
            "a201ce5132da4a879d276ba7b24b91b2",
            "efa523229fa641ddb74486fcee47ca6f",
            "96d011524c4c4ad0b171e4479e196dfb",
            "962d0ba416d04f31bd5de90f531d6035",
            "4aa3b419e05a42d482e9c87d6163049f",
            "3e42545a71b947e1905617d4054947df",
            "41f09db69385438db730e13a3472e565",
            "6ba79f10b05b4eec8bb95d54a0257edb",
            "cf7bca4832ca40958f42bcaede6a054d",
            "789d85aaad164e19ad5bfb592aeca3ba"
          ]
        },
        "id": "Mknn6gXQW_1s",
        "outputId": "d45863c9-d4f4-40ea-d9f9-7abd56dea2f7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "794989b18e6f4f07afb5ca4c67f5f77b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "def create_model():\n",
        "    tweet_input = Input(shape=(None,), dtype=tf.int32, name='tweet_input')\n",
        "    bert_model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "    bert_embeddings = bert_model(tweet_input)[0]\n",
        "    cnn_layer = Conv1D(64, kernel_size=3, activation='relu')(bert_embeddings)\n",
        "    cnn_layer = GlobalMaxPooling1D()(cnn_layer)\n",
        "\n",
        "\n",
        "    price_input = Input(shape=(len(price_features),), name='price_input')\n",
        "    lstm_price = LSTM(64, return_sequences=True)(tf.expand_dims(price_input, axis=1))\n",
        "    attention_price = Attention()([lstm_price, lstm_price])\n",
        "    lstm_price_output = LSTM(64)(attention_price)\n",
        "\n",
        "    tech_input = Input(shape=(len(tech_features),), name='tech_input')\n",
        "    lstm_tech = LSTM(64, return_sequences=True)(tf.expand_dims(tech_input, axis=1))\n",
        "    attention_tech = Attention()([lstm_tech, lstm_tech])\n",
        "    lstm_tech_output = LSTM(64)(attention_tech)\n",
        "\n",
        "    concatenated = Concatenate(axis=1)([cnn_layer, lstm_price_output, lstm_tech_output])\n",
        "    dense1 = Dense(64, activation='relu')(concatenated)\n",
        "    dropout = Dropout(0.5)(dense1)\n",
        "    output = Dense(3, activation='softmax')(dropout)\n",
        "\n",
        "    model = Model(inputs=[tweet_input, price_input, tech_input], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyYb3lC0qIR_",
        "outputId": "e2750b7d-f81d-41a7-b4a6-4edef7f740fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " price_input (InputLayer)    [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " tech_input (InputLayer)     [(None, 4)]                  0         []                            \n",
            "                                                                                                  \n",
            " tweet_input (InputLayer)    [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " tf.expand_dims (TFOpLambda  (None, 1, 2)                 0         ['price_input[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.expand_dims_1 (TFOpLamb  (None, 1, 4)                 0         ['tech_input[0][0]']          \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDi  TFBaseModelOutput(last_hid   1088901   ['tweet_input[0][0]']         \n",
            " stilBertModel)              den_state=(None, None, 768   12                                      \n",
            "                             ),                                                                   \n",
            "                              hidden_states=None, atten                                           \n",
            "                             tions=None)                                                          \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, 1, 64)                17152     ['tf.expand_dims[0][0]']      \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)               (None, 1, 64)                17664     ['tf.expand_dims_1[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, None, 64)             147520    ['tf_distil_bert_model[0][0]']\n",
            "                                                                                                  \n",
            " attention (Attention)       (None, 1, 64)                0         ['lstm[0][0]',                \n",
            "                                                                     'lstm[0][0]']                \n",
            "                                                                                                  \n",
            " attention_1 (Attention)     (None, 1, 64)                0         ['lstm_2[0][0]',              \n",
            "                                                                     'lstm_2[0][0]']              \n",
            "                                                                                                  \n",
            " global_max_pooling1d (Glob  (None, 64)                   0         ['conv1d[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               (None, 64)                   33024     ['attention[0][0]']           \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)               (None, 64)                   33024     ['attention_1[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 192)                  0         ['global_max_pooling1d[0][0]',\n",
            "                                                                     'lstm_1[0][0]',              \n",
            "                                                                     'lstm_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 64)                   12352     ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)        (None, 64)                   0         ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 3)                    195       ['dropout_37[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109151043 (416.38 MB)\n",
            "Trainable params: 109151043 (416.38 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_tweet_train = {k: v[:len(X_prices_train)] for k, v in tweet_encodings.items()}\n",
        "X_tweet_val = {k: v[len(X_prices_train):len(X_prices_train) + len(X_prices_val)] for k, v in tweet_encodings.items()}\n",
        "X_tweet_test = {k: v[len(X_prices_train) + len(X_prices_val):] for k, v in tweet_encodings.items()}\n",
        "\n",
        "history = model.fit(\n",
        "    [X_tweet_train['input_ids'], X_prices_train, X_tech_train], y_train,\n",
        "    epochs=50,\n",
        "    batch_size=16,\n",
        "    validation_data=([X_tweet_val['input_ids'], X_prices_val, X_tech_val], y_val)\n",
        ")\n",
        "\n",
        "loss, accuracy = model.evaluate([X_tweet_test['input_ids'], X_prices_test, X_tech_test], y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCJx1lbXq79Q",
        "outputId": "1b171681-4ccd-498f-a414-7ae8dc13abee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "50/50 [==============================] - 11s 229ms/step - loss: 0.7401 - accuracy: 0.5296 - val_loss: 0.7355 - val_accuracy: 0.4774\n",
            "Epoch 2/50\n",
            "50/50 [==============================] - 11s 223ms/step - loss: 0.7389 - accuracy: 0.4780 - val_loss: 0.7339 - val_accuracy: 0.4925\n",
            "Epoch 3/50\n",
            "50/50 [==============================] - 11s 224ms/step - loss: 0.7199 - accuracy: 0.4956 - val_loss: 0.7330 - val_accuracy: 0.4925\n",
            "Epoch 4/50\n",
            "50/50 [==============================] - 11s 220ms/step - loss: 0.7266 - accuracy: 0.4805 - val_loss: 0.7330 - val_accuracy: 0.4925\n",
            "Epoch 5/50\n",
            "50/50 [==============================] - 11s 222ms/step - loss: 0.7211 - accuracy: 0.5006 - val_loss: 0.7487 - val_accuracy: 0.4925\n",
            "Epoch 6/50\n",
            "50/50 [==============================] - 11s 219ms/step - loss: 0.7230 - accuracy: 0.4818 - val_loss: 0.7342 - val_accuracy: 0.4925\n",
            "Epoch 7/50\n",
            "50/50 [==============================] - 11s 211ms/step - loss: 0.7170 - accuracy: 0.5346 - val_loss: 0.7433 - val_accuracy: 0.4925\n",
            "Epoch 8/50\n",
            "50/50 [==============================] - 10s 207ms/step - loss: 0.7219 - accuracy: 0.5119 - val_loss: 0.7441 - val_accuracy: 0.4925\n",
            "Epoch 9/50\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.7145 - accuracy: 0.5220 - val_loss: 0.7467 - val_accuracy: 0.4925\n",
            "Epoch 10/50\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.7116 - accuracy: 0.5094 - val_loss: 0.7466 - val_accuracy: 0.4925\n",
            "Epoch 11/50\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.7095 - accuracy: 0.5447 - val_loss: 0.7431 - val_accuracy: 0.4925\n",
            "Epoch 12/50\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.7170 - accuracy: 0.4918 - val_loss: 0.7418 - val_accuracy: 0.4925\n",
            "Epoch 13/50\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.7185 - accuracy: 0.4943 - val_loss: 0.7403 - val_accuracy: 0.4925\n",
            "Epoch 14/50\n",
            "50/50 [==============================] - 10s 207ms/step - loss: 0.7091 - accuracy: 0.5421 - val_loss: 0.7441 - val_accuracy: 0.4925\n",
            "Epoch 15/50\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.7143 - accuracy: 0.5296 - val_loss: 0.7472 - val_accuracy: 0.4812\n",
            "Epoch 16/50\n",
            "50/50 [==============================] - 10s 207ms/step - loss: 0.7125 - accuracy: 0.5044 - val_loss: 0.7447 - val_accuracy: 0.4925\n",
            "Epoch 17/50\n",
            "50/50 [==============================] - 10s 207ms/step - loss: 0.7153 - accuracy: 0.5371 - val_loss: 0.7453 - val_accuracy: 0.4925\n",
            "Epoch 18/50\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.7133 - accuracy: 0.4981 - val_loss: 0.7465 - val_accuracy: 0.4925\n",
            "Epoch 19/50\n",
            "50/50 [==============================] - 10s 207ms/step - loss: 0.7072 - accuracy: 0.5283 - val_loss: 0.7492 - val_accuracy: 0.4925\n",
            "Epoch 20/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7147 - accuracy: 0.5145 - val_loss: 0.7485 - val_accuracy: 0.4925\n",
            "Epoch 21/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7150 - accuracy: 0.5157 - val_loss: 0.7482 - val_accuracy: 0.4925\n",
            "Epoch 22/50\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.7114 - accuracy: 0.5119 - val_loss: 0.7519 - val_accuracy: 0.4925\n",
            "Epoch 23/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7171 - accuracy: 0.5082 - val_loss: 0.7451 - val_accuracy: 0.4925\n",
            "Epoch 24/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7092 - accuracy: 0.5195 - val_loss: 0.7481 - val_accuracy: 0.4925\n",
            "Epoch 25/50\n",
            "50/50 [==============================] - 10s 208ms/step - loss: 0.7165 - accuracy: 0.5119 - val_loss: 0.7501 - val_accuracy: 0.4925\n",
            "Epoch 26/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7117 - accuracy: 0.5333 - val_loss: 0.7488 - val_accuracy: 0.4925\n",
            "Epoch 27/50\n",
            "50/50 [==============================] - 10s 208ms/step - loss: 0.7080 - accuracy: 0.5396 - val_loss: 0.7502 - val_accuracy: 0.4925\n",
            "Epoch 28/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7104 - accuracy: 0.5182 - val_loss: 0.7503 - val_accuracy: 0.4925\n",
            "Epoch 29/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7104 - accuracy: 0.5208 - val_loss: 0.7513 - val_accuracy: 0.4925\n",
            "Epoch 30/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7069 - accuracy: 0.5069 - val_loss: 0.7480 - val_accuracy: 0.4925\n",
            "Epoch 31/50\n",
            "50/50 [==============================] - 10s 207ms/step - loss: 0.7087 - accuracy: 0.5233 - val_loss: 0.7510 - val_accuracy: 0.4925\n",
            "Epoch 32/50\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.7094 - accuracy: 0.5132 - val_loss: 0.7510 - val_accuracy: 0.4925\n",
            "Epoch 33/50\n",
            "50/50 [==============================] - 10s 207ms/step - loss: 0.7098 - accuracy: 0.5270 - val_loss: 0.7497 - val_accuracy: 0.4925\n",
            "Epoch 34/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7115 - accuracy: 0.5094 - val_loss: 0.7487 - val_accuracy: 0.4925\n",
            "Epoch 35/50\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.7166 - accuracy: 0.5082 - val_loss: 0.7477 - val_accuracy: 0.4925\n",
            "Epoch 36/50\n",
            "50/50 [==============================] - 10s 208ms/step - loss: 0.7150 - accuracy: 0.5308 - val_loss: 0.7480 - val_accuracy: 0.4925\n",
            "Epoch 37/50\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.7084 - accuracy: 0.5346 - val_loss: 0.7497 - val_accuracy: 0.4925\n",
            "Epoch 38/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7101 - accuracy: 0.5082 - val_loss: 0.7478 - val_accuracy: 0.4925\n",
            "Epoch 39/50\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.7170 - accuracy: 0.5384 - val_loss: 0.7507 - val_accuracy: 0.4925\n",
            "Epoch 40/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7004 - accuracy: 0.5384 - val_loss: 0.7549 - val_accuracy: 0.4925\n",
            "Epoch 41/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7092 - accuracy: 0.5296 - val_loss: 0.7542 - val_accuracy: 0.4925\n",
            "Epoch 42/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7092 - accuracy: 0.5371 - val_loss: 0.7523 - val_accuracy: 0.4812\n",
            "Epoch 43/50\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.7126 - accuracy: 0.5321 - val_loss: 0.7511 - val_accuracy: 0.4925\n",
            "Epoch 44/50\n",
            "50/50 [==============================] - 10s 204ms/step - loss: 0.7117 - accuracy: 0.5371 - val_loss: 0.7478 - val_accuracy: 0.4925\n",
            "Epoch 45/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7101 - accuracy: 0.5308 - val_loss: 0.7515 - val_accuracy: 0.4925\n",
            "Epoch 46/50\n",
            "50/50 [==============================] - 10s 203ms/step - loss: 0.7077 - accuracy: 0.5132 - val_loss: 0.7510 - val_accuracy: 0.4925\n",
            "Epoch 47/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7025 - accuracy: 0.5044 - val_loss: 0.7510 - val_accuracy: 0.4925\n",
            "Epoch 48/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7099 - accuracy: 0.5283 - val_loss: 0.7481 - val_accuracy: 0.4925\n",
            "Epoch 49/50\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.7071 - accuracy: 0.5497 - val_loss: 0.7492 - val_accuracy: 0.4925\n",
            "Epoch 50/50\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.7119 - accuracy: 0.5107 - val_loss: 0.7496 - val_accuracy: 0.4925\n",
            "9/9 [==============================] - 1s 110ms/step - loss: 0.6914 - accuracy: 0.5113\n",
            "Test Loss: 0.691371738910675, Test Accuracy: 0.5112782120704651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AMZN\n"
      ],
      "metadata": {
        "id": "v9R2mrxYLj4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Jh3qAMUZJcWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_data_by_stock = pd.read_csv('/content/drive/MyDrive/dldata/tweets_h1_2015.csv')"
      ],
      "metadata": {
        "id": "iJY4gtXwLxyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_data_by_stock.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2BEEumSPL9Lx",
        "outputId": "e251faf0-4f8a-4c76-ef48-edb759c507fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        post_date            tweet_id  \\\n",
              "371681  2015-06-01 20:51:55-04:00  605537290815832065   \n",
              "371682  2015-06-01 20:52:21-04:00  605537403017691136   \n",
              "371683  2015-06-01 20:52:23-04:00  605537411385323520   \n",
              "371684  2015-06-01 20:52:45-04:00  605537501789356032   \n",
              "371685  2015-06-01 20:58:28-04:00  605538940712448000   \n",
              "\n",
              "                      post_date.1  \\\n",
              "371681  2015-06-01 20:51:55-04:00   \n",
              "371682  2015-06-01 20:52:21-04:00   \n",
              "371683  2015-06-01 20:52:23-04:00   \n",
              "371684  2015-06-01 20:52:45-04:00   \n",
              "371685  2015-06-01 20:58:28-04:00   \n",
              "\n",
              "                                                     body ticker_symbol  \n",
              "371681  Our Penny Stock Alerts Gained Over 3400% For S...          GOOG  \n",
              "371682  Our Penny Stock Alerts Gained Over 3400% For S...          MSFT  \n",
              "371683  Great get by $AMZN. Congrats @albert_cheng on ...          AMZN  \n",
              "371684  Our Penny Stock Alerts Gained Over 3400% For S...          AMZN  \n",
              "371685  The 6 pro traders at http://StockAviator.com a...          MSFT  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c0c7e41-9f75-4beb-87a6-73d523f83756\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_date</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>post_date.1</th>\n",
              "      <th>body</th>\n",
              "      <th>ticker_symbol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>371681</th>\n",
              "      <td>2015-06-01 20:51:55-04:00</td>\n",
              "      <td>605537290815832065</td>\n",
              "      <td>2015-06-01 20:51:55-04:00</td>\n",
              "      <td>Our Penny Stock Alerts Gained Over 3400% For S...</td>\n",
              "      <td>GOOG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371682</th>\n",
              "      <td>2015-06-01 20:52:21-04:00</td>\n",
              "      <td>605537403017691136</td>\n",
              "      <td>2015-06-01 20:52:21-04:00</td>\n",
              "      <td>Our Penny Stock Alerts Gained Over 3400% For S...</td>\n",
              "      <td>MSFT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371683</th>\n",
              "      <td>2015-06-01 20:52:23-04:00</td>\n",
              "      <td>605537411385323520</td>\n",
              "      <td>2015-06-01 20:52:23-04:00</td>\n",
              "      <td>Great get by $AMZN. Congrats @albert_cheng on ...</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371684</th>\n",
              "      <td>2015-06-01 20:52:45-04:00</td>\n",
              "      <td>605537501789356032</td>\n",
              "      <td>2015-06-01 20:52:45-04:00</td>\n",
              "      <td>Our Penny Stock Alerts Gained Over 3400% For S...</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371685</th>\n",
              "      <td>2015-06-01 20:58:28-04:00</td>\n",
              "      <td>605538940712448000</td>\n",
              "      <td>2015-06-01 20:58:28-04:00</td>\n",
              "      <td>The 6 pro traders at http://StockAviator.com a...</td>\n",
              "      <td>MSFT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c0c7e41-9f75-4beb-87a6-73d523f83756')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8c0c7e41-9f75-4beb-87a6-73d523f83756 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8c0c7e41-9f75-4beb-87a6-73d523f83756');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9ac48515-45b0-49e3-b7fa-d868d9411cb8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9ac48515-45b0-49e3-b7fa-d868d9411cb8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9ac48515-45b0-49e3-b7fa-d868d9411cb8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_data_by_stock = {stock: df for stock, df in tweet_data_by_stock.groupby('ticker_symbol')}"
      ],
      "metadata": {
        "id": "uNXPE3d3MEnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amzn = tweet_data_by_stock['AMZN']\n",
        "\n",
        "amzn['datetime'] = pd.to_datetime(amzn['post_date'], utc=True)\n",
        "amzn['datetime'] = amzn['datetime'].dt.tz_convert('America/New_York')\n",
        "amzn_aggregated = amzn.groupby(amzn['datetime'].dt.floor('H'))['body'].apply(' '.join).reset_index()\n",
        "amzn_aggregated.columns = ['datetime', 'aggregated_tweet']"
      ],
      "metadata": {
        "id": "Zt8PRh5qLxXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices = pd.read_csv('/content/drive/MyDrive/dldata/h1_2015_AMZN_prices_tech.csv')"
      ],
      "metadata": {
        "id": "PmC-8OABMgYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices['datetime'] = pd.to_datetime(prices['TIME'], utc=True)\n",
        "prices['datetime'] = prices['datetime'].dt.tz_convert('America/New_York')"
      ],
      "metadata": {
        "id": "w4R4bihoMoIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = pd.merge(prices, amzn_aggregated, left_on='datetime', right_on='datetime')"
      ],
      "metadata": {
        "id": "N7-68XAAMraY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tech = prices[['returns','SP-EMA5','OBV','MACD']]"
      ],
      "metadata": {
        "id": "lKGDD2R2MzqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = merged.dropna(subset=['returns', 'PRICE'])"
      ],
      "metadata": {
        "id": "RqE91v09MzqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_features = ['PRICE', 'SIZE']\n",
        "tech_features = ['returns','SP-EMA5','OBV','MACD']\n",
        "X_prices = merged[price_features].values\n",
        "X_tech = merged[tech_features].values\n",
        "\n",
        "y = merged['return_label'].shift(-1).dropna()\n",
        "X_prices = X_prices[:-1]\n",
        "X_tech = X_tech[:-1]"
      ],
      "metadata": {
        "id": "gL8gRQ3qMzqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Concatenate, Conv1D, GlobalMaxPooling1D, Dropout, Attention, Reshape\n",
        "from transformers import TFDistilBertModel, DistilBertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
      ],
      "metadata": {
        "id": "lmuE6XF9MzqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(y)\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "y_encoded = onehot_encoder.fit_transform(integer_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7142fc0-48d6-48db-bc8a-97de47e661b2",
        "id": "ytPyg8R2MzqE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "max_seq_length = 128\n",
        "tweet_texts = merged['aggregated_tweet'].values\n",
        "tweet_encodings = tokenizer(list(tweet_texts[:-1]), truncation=True, padding=True, max_length=max_seq_length, return_tensors=\"tf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f297aceb-f7c5-4e03-e588-a29f353c8cdc",
        "id": "pOJm1hSmMzqE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tweet_encodings['input_ids']\n",
        "attention_mask = tweet_encodings['attention_mask']\n",
        "#token_type_ids = tweet_encodings['token_type_ids']"
      ],
      "metadata": {
        "id": "h0FvDF1NNL3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_prices_train, X_prices_test, X_tech_train, X_tech_test, y_train, y_test = train_test_split(\n",
        "    X_prices, X_tech, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_prices_train, X_prices_val, X_tech_train, X_tech_val, y_train, y_val = train_test_split(\n",
        "    X_prices_train, X_tech_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
      ],
      "metadata": {
        "id": "x9VuStPJNL3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.37.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "collapsed": true,
        "id": "Uk1N5nwlNxWc",
        "outputId": "acddf4a5-0410-4708-919f-e8c961a98431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.37.2\n",
            "  Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.37.2)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2024.2.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.0\n",
            "    Uninstalling transformers-4.41.0:\n",
            "      Successfully uninstalled transformers-4.41.0\n",
            "Successfully installed tokenizers-0.15.2 transformers-4.37.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "449cf498aec4405fb2d0aea9161c2019"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e60584-2d15-4d68-f144-96317258b0e3",
        "id": "fNNPmEmXNL3J"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'cls.predictions.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertModel were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.11.output_layer_norm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "def create_model():\n",
        "    tweet_input = Input(shape=(None,), dtype=tf.int32, name='tweet_input')\n",
        "    bert_model = TFDistilBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    bert_embeddings = bert_model(tweet_input)[0]\n",
        "    cnn_layer = Conv1D(64, kernel_size=3, activation='relu')(bert_embeddings)\n",
        "    cnn_layer = GlobalMaxPooling1D()(cnn_layer)\n",
        "\n",
        "\n",
        "    price_input = Input(shape=(len(price_features),), name='price_input')\n",
        "    lstm_price = LSTM(64, return_sequences=True)(tf.expand_dims(price_input, axis=1))\n",
        "    attention_price = Attention()([lstm_price, lstm_price])\n",
        "    lstm_price_output = LSTM(64)(attention_price)\n",
        "\n",
        "    tech_input = Input(shape=(len(tech_features),), name='tech_input')\n",
        "    lstm_tech = LSTM(64, return_sequences=True)(tf.expand_dims(tech_input, axis=1))\n",
        "    attention_tech = Attention()([lstm_tech, lstm_tech])\n",
        "    lstm_tech_output = LSTM(64)(attention_tech)\n",
        "\n",
        "    concatenated = Concatenate(axis=1)([cnn_layer, lstm_price_output, lstm_tech_output])\n",
        "    dense1 = Dense(64, activation='relu')(concatenated)\n",
        "    dropout = Dropout(0.5)(dense1)\n",
        "    output = Dense(3, activation='softmax')(dropout)\n",
        "\n",
        "    model = Model(inputs=[tweet_input, price_input, tech_input], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "4668c080-61f5-43fc-b9a7-795d1c9eb8d1",
        "id": "9YUn6nb3NL3J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5f15418b3570>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_tweet_train = {k: v[:len(X_prices_train)] for k, v in tweet_encodings.items()}\n",
        "X_tweet_val = {k: v[len(X_prices_train):len(X_prices_train) + len(X_prices_val)] for k, v in tweet_encodings.items()}\n",
        "X_tweet_test = {k: v[len(X_prices_train) + len(X_prices_val):] for k, v in tweet_encodings.items()}\n",
        "\n",
        "history = model.fit(\n",
        "    [X_tweet_train['input_ids'], X_prices_train, X_tech_train], y_train,\n",
        "    epochs=15,\n",
        "    batch_size=16,\n",
        "    validation_data=([X_tweet_val['input_ids'], X_prices_val, X_tech_val], y_val)\n",
        ")\n",
        "\n",
        "loss, accuracy = model.evaluate([X_tweet_test['input_ids'], X_prices_test, X_tech_test], y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b6f6f841-90ff-42ec-858e-87a0cc0795fe",
        "id": "9hVzmIgONL3J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 65s 333ms/step - loss: 1.0993 - accuracy: 0.4968 - val_loss: 0.7771 - val_accuracy: 0.5326\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 11s 234ms/step - loss: 0.8266 - accuracy: 0.4891 - val_loss: 0.7519 - val_accuracy: 0.4713\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 11s 226ms/step - loss: 0.7778 - accuracy: 0.4981 - val_loss: 0.7090 - val_accuracy: 0.4559\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 11s 225ms/step - loss: 0.7744 - accuracy: 0.4802 - val_loss: 0.7445 - val_accuracy: 0.5134\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 10s 214ms/step - loss: 0.7580 - accuracy: 0.5211 - val_loss: 0.7052 - val_accuracy: 0.5326\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 10s 211ms/step - loss: 0.7575 - accuracy: 0.4815 - val_loss: 0.7095 - val_accuracy: 0.5326\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 10s 213ms/step - loss: 0.7382 - accuracy: 0.5096 - val_loss: 0.7040 - val_accuracy: 0.5326\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 10s 215ms/step - loss: 0.7351 - accuracy: 0.5121 - val_loss: 0.7089 - val_accuracy: 0.4559\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 10s 211ms/step - loss: 0.7328 - accuracy: 0.4994 - val_loss: 0.7082 - val_accuracy: 0.5326\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 10s 210ms/step - loss: 0.7372 - accuracy: 0.5032 - val_loss: 0.7120 - val_accuracy: 0.4789\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 10s 211ms/step - loss: 0.7291 - accuracy: 0.4879 - val_loss: 0.7187 - val_accuracy: 0.4330\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 10s 209ms/step - loss: 0.7274 - accuracy: 0.4968 - val_loss: 0.7074 - val_accuracy: 0.5326\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 10s 209ms/step - loss: 0.7334 - accuracy: 0.5198 - val_loss: 0.7220 - val_accuracy: 0.4368\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 10s 208ms/step - loss: 0.7195 - accuracy: 0.5211 - val_loss: 0.7086 - val_accuracy: 0.5326\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 10s 210ms/step - loss: 0.7203 - accuracy: 0.4981 - val_loss: 0.7147 - val_accuracy: 0.4444\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 10s 209ms/step - loss: 0.7287 - accuracy: 0.5275 - val_loss: 0.7139 - val_accuracy: 0.4253\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 10s 208ms/step - loss: 0.7272 - accuracy: 0.5006 - val_loss: 0.7089 - val_accuracy: 0.5249\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 10s 208ms/step - loss: 0.7223 - accuracy: 0.5109 - val_loss: 0.7190 - val_accuracy: 0.4330\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 10s 210ms/step - loss: 0.7357 - accuracy: 0.5109 - val_loss: 0.7157 - val_accuracy: 0.4330\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 10s 209ms/step - loss: 0.7222 - accuracy: 0.5096 - val_loss: 0.7116 - val_accuracy: 0.4751\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 10s 208ms/step - loss: 0.7210 - accuracy: 0.4840 - val_loss: 0.7084 - val_accuracy: 0.4521\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 10s 208ms/step - loss: 0.7278 - accuracy: 0.5019 - val_loss: 0.7161 - val_accuracy: 0.4636\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 10s 207ms/step - loss: 0.7260 - accuracy: 0.4713 - val_loss: 0.7077 - val_accuracy: 0.4943\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 10s 208ms/step - loss: 0.7187 - accuracy: 0.5172 - val_loss: 0.7150 - val_accuracy: 0.4444\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 10s 207ms/step - loss: 0.7231 - accuracy: 0.5338 - val_loss: 0.7115 - val_accuracy: 0.4636\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 10s 208ms/step - loss: 0.7181 - accuracy: 0.5109 - val_loss: 0.7097 - val_accuracy: 0.4636\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 10s 207ms/step - loss: 0.7354 - accuracy: 0.5134 - val_loss: 0.7146 - val_accuracy: 0.5134\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 10s 208ms/step - loss: 0.7246 - accuracy: 0.5057 - val_loss: 0.7200 - val_accuracy: 0.4444\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 10s 209ms/step - loss: 0.7203 - accuracy: 0.5019 - val_loss: 0.7106 - val_accuracy: 0.4559\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 10s 208ms/step - loss: 0.7213 - accuracy: 0.4994 - val_loss: 0.7111 - val_accuracy: 0.4483\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 10s 207ms/step - loss: 0.7266 - accuracy: 0.4853 - val_loss: 0.7108 - val_accuracy: 0.4521\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 10s 209ms/step - loss: 0.7262 - accuracy: 0.5185 - val_loss: 0.7133 - val_accuracy: 0.4444\n",
            "Epoch 33/50\n",
            "39/49 [======================>.......] - ETA: 1s - loss: 0.7097 - accuracy: 0.5080"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-91cef795fda0>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_tweet_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_prices_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_prices_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet_encodings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mX_tweet_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_prices_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tech_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate([X_tweet_test['input_ids'], X_prices_test, X_tech_test], y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abJvArP7PlGh",
        "outputId": "df8547f9-4b88-4b90-9687-5664ae5e2959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 1s 111ms/step - loss: 0.7050 - accuracy: 0.4368\n",
            "Test Loss: 0.7049642205238342, Test Accuracy: 0.4367816150188446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AAPL"
      ],
      "metadata": {
        "id": "KfifH8WoPxD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aapl = tweet_data_by_stock['AAPL']\n",
        "\n",
        "aapl['datetime'] = pd.to_datetime(aapl['post_date'], utc=True)\n",
        "\n",
        "# Convert to local time zone if needed (e.g., US Eastern Time)\n",
        "aapl['datetime'] = aapl['datetime'].dt.tz_convert('America/New_York')\n",
        "\n",
        "# Aggregate tweets by hour\n",
        "aapl_aggregated = aapl.groupby(aapl['datetime'].dt.floor('H'))['body'].apply(' '.join).reset_index()\n",
        "aapl_aggregated.columns = ['datetime', 'aggregated_tweet']"
      ],
      "metadata": {
        "id": "aUEqFTuHP4ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices = pd.read_csv('/content/drive/MyDrive/dldata/h1_2015_AAPL_prices_tech.csv')"
      ],
      "metadata": {
        "id": "HptdwUjUP4ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices['datetime'] = pd.to_datetime(prices['TIME'], utc=True)\n",
        "prices['datetime'] = prices['datetime'].dt.tz_convert('America/New_York')"
      ],
      "metadata": {
        "id": "H721yKqrP4ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = pd.merge(prices, amzn_aggregated, left_on='datetime', right_on='datetime')"
      ],
      "metadata": {
        "id": "uFzRDpoeP4ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tech = prices[['returns','SP-EMA5','OBV','MACD']]"
      ],
      "metadata": {
        "id": "2yfmeRZgP4ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = merged.dropna(subset=['returns', 'PRICE'])"
      ],
      "metadata": {
        "id": "Vym_h_I-P4ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_features = ['PRICE', 'SIZE']\n",
        "tech_features = ['returns','SP-EMA5','OBV','MACD']\n",
        "X_prices = merged[price_features].values\n",
        "X_tech = merged[tech_features].values\n",
        "\n",
        "y = merged['return_label'].shift(-1).dropna()\n",
        "X_prices = X_prices[:-1]\n",
        "X_tech = X_tech[:-1]"
      ],
      "metadata": {
        "id": "z7ANM2QSP4ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(y)\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "y_encoded = onehot_encoder.fit_transform(integer_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7daeaefa-bd56-4290-c5ee-f5d894c6b5b6",
        "id": "L0L4zIHxP4ig"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_texts = merged['aggregated_tweet'].values  # Assuming 'tweets' column has the aggregated tweets\n",
        "tweet_encodings = tokenizer(list(tweet_texts[:-1]), truncation=True, padding=True, max_length=max_seq_length, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "XYUBpMnbP4ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tweet_encodings['input_ids']\n",
        "attention_mask = tweet_encodings['attention_mask']\n",
        "#token_type_ids = tweet_encodings['token_type_ids']"
      ],
      "metadata": {
        "id": "uGUQIq8sP4ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_prices_train, X_prices_test, X_tech_train, X_tech_test, y_train, y_test = train_test_split(\n",
        "    X_prices, X_tech, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_prices_train, X_prices_val, X_tech_train, X_tech_val, y_train, y_val = train_test_split(\n",
        "    X_prices_train, X_tech_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
      ],
      "metadata": {
        "id": "1DNbH_F6P4ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8786fdd8-97db-40b8-9c53-b6e56c52ebc6",
        "id": "zFfuuPU9P4ih"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'cls.predictions.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertModel were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.11.output_layer_norm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "def create_model():\n",
        "    tweet_input = Input(shape=(None,), dtype=tf.int32, name='tweet_input')\n",
        "    bert_model = TFDistilBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    bert_embeddings = bert_model(tweet_input)[0]\n",
        "    cnn_layer = Conv1D(64, kernel_size=3, activation='relu')(bert_embeddings)\n",
        "    cnn_layer = GlobalMaxPooling1D()(cnn_layer)\n",
        "\n",
        "\n",
        "    price_input = Input(shape=(len(price_features),), name='price_input')\n",
        "    lstm_price = LSTM(64, return_sequences=True)(tf.expand_dims(price_input, axis=1))\n",
        "    attention_price = Attention()([lstm_price, lstm_price])\n",
        "    lstm_price_output = LSTM(64)(attention_price)\n",
        "\n",
        "    tech_input = Input(shape=(len(tech_features),), name='tech_input')\n",
        "    lstm_tech = LSTM(64, return_sequences=True)(tf.expand_dims(tech_input, axis=1))\n",
        "    attention_tech = Attention()([lstm_tech, lstm_tech])\n",
        "    lstm_tech_output = LSTM(64)(attention_tech)\n",
        "\n",
        "    concatenated = Concatenate(axis=1)([cnn_layer, lstm_price_output, lstm_tech_output])\n",
        "    dense1 = Dense(64, activation='relu')(concatenated)\n",
        "    dropout = Dropout(0.5)(dense1)\n",
        "    output = Dense(3, activation='softmax')(dropout)\n",
        "\n",
        "    model = Model(inputs=[tweet_input, price_input, tech_input], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "4668c080-61f5-43fc-b9a7-795d1c9eb8d1",
        "id": "qOlusRgVP4ih"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5f15418b3570>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_tweet_train = {k: v[:len(X_prices_train)] for k, v in tweet_encodings.items()}\n",
        "X_tweet_val = {k: v[len(X_prices_train):len(X_prices_train) + len(X_prices_val)] for k, v in tweet_encodings.items()}\n",
        "X_tweet_test = {k: v[len(X_prices_train) + len(X_prices_val):] for k, v in tweet_encodings.items()}\n",
        "\n",
        "history = model.fit(\n",
        "    [X_tweet_train['input_ids'], X_prices_train, X_tech_train], y_train,\n",
        "    epochs=15,\n",
        "    batch_size=16,\n",
        "    validation_data=([X_tweet_val['input_ids'], X_prices_val, X_tech_val], y_val)\n",
        ")\n",
        "\n",
        "loss, accuracy = model.evaluate([X_tweet_test['input_ids'], X_prices_test, X_tech_test], y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42424597-af3c-4d66-ae2e-b2df09434cd8",
        "id": "KYUOS3ElP4ih"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "57/57 [==============================] - 62s 316ms/step - loss: 0.9979 - accuracy: 0.4890 - val_loss: 0.8117 - val_accuracy: 0.5347\n",
            "Epoch 2/15\n",
            "57/57 [==============================] - 13s 226ms/step - loss: 0.7536 - accuracy: 0.5099 - val_loss: 0.7022 - val_accuracy: 0.5347\n",
            "Epoch 3/15\n",
            "57/57 [==============================] - 13s 228ms/step - loss: 0.7194 - accuracy: 0.5033 - val_loss: 0.6962 - val_accuracy: 0.5347\n",
            "Epoch 4/15\n",
            "57/57 [==============================] - 13s 220ms/step - loss: 0.7148 - accuracy: 0.5077 - val_loss: 0.7033 - val_accuracy: 0.4653\n",
            "Epoch 5/15\n",
            "57/57 [==============================] - 12s 214ms/step - loss: 0.7157 - accuracy: 0.4901 - val_loss: 0.6952 - val_accuracy: 0.5347\n",
            "Epoch 6/15\n",
            "57/57 [==============================] - 12s 213ms/step - loss: 0.7117 - accuracy: 0.5055 - val_loss: 0.6940 - val_accuracy: 0.5347\n",
            "Epoch 7/15\n",
            "57/57 [==============================] - 12s 212ms/step - loss: 0.7052 - accuracy: 0.5187 - val_loss: 0.6933 - val_accuracy: 0.5347\n",
            "Epoch 8/15\n",
            "57/57 [==============================] - 12s 210ms/step - loss: 0.7048 - accuracy: 0.5154 - val_loss: 0.6943 - val_accuracy: 0.5347\n",
            "Epoch 9/15\n",
            "57/57 [==============================] - 12s 209ms/step - loss: 0.7077 - accuracy: 0.5110 - val_loss: 0.6943 - val_accuracy: 0.5347\n",
            "Epoch 10/15\n",
            "57/57 [==============================] - 12s 209ms/step - loss: 0.7055 - accuracy: 0.5033 - val_loss: 0.6934 - val_accuracy: 0.5347\n",
            "Epoch 11/15\n",
            "57/57 [==============================] - 12s 208ms/step - loss: 0.7052 - accuracy: 0.5187 - val_loss: 0.6940 - val_accuracy: 0.5347\n",
            "Epoch 12/15\n",
            "57/57 [==============================] - 12s 211ms/step - loss: 0.7092 - accuracy: 0.5165 - val_loss: 0.6931 - val_accuracy: 0.5347\n",
            "Epoch 13/15\n",
            "57/57 [==============================] - 12s 210ms/step - loss: 0.7045 - accuracy: 0.5121 - val_loss: 0.6931 - val_accuracy: 0.5347\n",
            "Epoch 14/15\n",
            "57/57 [==============================] - 12s 210ms/step - loss: 0.7014 - accuracy: 0.5110 - val_loss: 0.6931 - val_accuracy: 0.5347\n",
            "Epoch 15/15\n",
            "57/57 [==============================] - 12s 210ms/step - loss: 0.7031 - accuracy: 0.5000 - val_loss: 0.6946 - val_accuracy: 0.5347\n",
            "10/10 [==============================] - 1s 115ms/step - loss: 0.7191 - accuracy: 0.5314\n",
            "Test Loss: 0.7190715670585632, Test Accuracy: 0.5313531160354614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate([X_tweet_test['input_ids'], X_prices_test, X_tech_test], y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df8547f9-4b88-4b90-9687-5664ae5e2959",
        "id": "Sb1XYrrQP4ih"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 1s 111ms/step - loss: 0.7050 - accuracy: 0.4368\n",
            "Test Loss: 0.7049642205238342, Test Accuracy: 0.4367816150188446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$TSLA"
      ],
      "metadata": {
        "id": "rs4IXxOpRRc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsla = tweet_data_by_stock['TSLA']\n",
        "\n",
        "tsla['datetime'] = pd.to_datetime(tsla['post_date'], utc=True)\n",
        "\n",
        "# Convert to local time zone if needed (e.g., US Eastern Time)\n",
        "tsla['datetime'] = tsla['datetime'].dt.tz_convert('America/New_York')\n",
        "\n",
        "# Aggregate tweets by hour\n",
        "tsla_aggregated = tsla.groupby(tsla['datetime'].dt.floor('H'))['body'].apply(' '.join).reset_index()\n",
        "tsla_aggregated.columns = ['datetime', 'aggregated_tweet']"
      ],
      "metadata": {
        "id": "YAU7KBibResx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices = pd.read_csv('/content/drive/MyDrive/dldata/h1_2015_TSLA_prices_tech.csv')"
      ],
      "metadata": {
        "id": "gjMs-sMuResx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices['datetime'] = pd.to_datetime(prices['TIME'], utc=True)\n",
        "prices['datetime'] = prices['datetime'].dt.tz_convert('America/New_York')"
      ],
      "metadata": {
        "id": "7SkZ8DLZResx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = pd.merge(prices, amzn_aggregated, left_on='datetime', right_on='datetime')"
      ],
      "metadata": {
        "id": "GgznTEfNResx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tech = prices[['returns','SP-EMA5','OBV','MACD']]"
      ],
      "metadata": {
        "id": "E6hMsb8zResy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = merged.dropna(subset=['returns', 'PRICE'])"
      ],
      "metadata": {
        "id": "TY6gYbJwResy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_features = ['PRICE', 'SIZE']\n",
        "tech_features = ['returns','SP-EMA5','OBV','MACD']\n",
        "X_prices = merged[price_features].values\n",
        "X_tech = merged[tech_features].values\n",
        "\n",
        "y = merged['return_label'].shift(-1).dropna()\n",
        "X_prices = X_prices[:-1]\n",
        "X_tech = X_tech[:-1]"
      ],
      "metadata": {
        "id": "uCMJRlE1Resy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(y)\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "y_encoded = onehot_encoder.fit_transform(integer_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de427ef-ef13-4176-8885-88376279752d",
        "id": "9d9Jn8HyResy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_texts = merged['aggregated_tweet'].values  # Assuming 'tweets' column has the aggregated tweets\n",
        "tweet_encodings = tokenizer(list(tweet_texts[:-1]), truncation=True, padding=True, max_length=max_seq_length, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "-gvhQF0FResy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tweet_encodings['input_ids']\n",
        "attention_mask = tweet_encodings['attention_mask']\n",
        "#token_type_ids = tweet_encodings['token_type_ids']"
      ],
      "metadata": {
        "id": "8kd4OTjKResz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_prices_train, X_prices_test, X_tech_train, X_tech_test, y_train, y_test = train_test_split(\n",
        "    X_prices, X_tech, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_prices_train, X_prices_val, X_tech_train, X_tech_val, y_train, y_val = train_test_split(\n",
        "    X_prices_train, X_tech_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
      ],
      "metadata": {
        "id": "dbdBMkBMResz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9725e197-e545-4cc9-cd70-850ce2080190",
        "id": "PIVZsDdJResz"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'cls.predictions.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertModel were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.11.output_layer_norm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "def create_model():\n",
        "    tweet_input = Input(shape=(None,), dtype=tf.int32, name='tweet_input')\n",
        "    bert_model = TFDistilBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    bert_embeddings = bert_model(tweet_input)[0]\n",
        "    cnn_layer = Conv1D(64, kernel_size=3, activation='relu')(bert_embeddings)\n",
        "    cnn_layer = GlobalMaxPooling1D()(cnn_layer)\n",
        "\n",
        "\n",
        "    price_input = Input(shape=(len(price_features),), name='price_input')\n",
        "    lstm_price = LSTM(64, return_sequences=True)(tf.expand_dims(price_input, axis=1))\n",
        "    attention_price = Attention()([lstm_price, lstm_price])\n",
        "    lstm_price_output = LSTM(64)(attention_price)\n",
        "\n",
        "    tech_input = Input(shape=(len(tech_features),), name='tech_input')\n",
        "    lstm_tech = LSTM(64, return_sequences=True)(tf.expand_dims(tech_input, axis=1))\n",
        "    attention_tech = Attention()([lstm_tech, lstm_tech])\n",
        "    lstm_tech_output = LSTM(64)(attention_tech)\n",
        "\n",
        "    concatenated = Concatenate(axis=1)([cnn_layer, lstm_price_output, lstm_tech_output])\n",
        "    dense1 = Dense(64, activation='relu')(concatenated)\n",
        "    dropout = Dropout(0.5)(dense1)\n",
        "    output = Dense(3, activation='softmax')(dropout)\n",
        "\n",
        "    model = Model(inputs=[tweet_input, price_input, tech_input], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85a5d123-8a69-4b3c-8e57-561162c22fee",
        "id": "D53yiwtTResz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " price_input (InputLayer)    [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " tech_input (InputLayer)     [(None, 4)]                  0         []                            \n",
            "                                                                                                  \n",
            " tweet_input (InputLayer)    [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " tf.expand_dims_4 (TFOpLamb  (None, 1, 2)                 0         ['price_input[0][0]']         \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.expand_dims_5 (TFOpLamb  (None, 1, 4)                 0         ['tech_input[0][0]']          \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf_distil_bert_model_2 (TF  TFBaseModelOutput(last_hid   1088901   ['tweet_input[0][0]']         \n",
            " DistilBertModel)            den_state=(None, None, 768   12                                      \n",
            "                             ),                                                                   \n",
            "                              hidden_states=None, atten                                           \n",
            "                             tions=None)                                                          \n",
            "                                                                                                  \n",
            " lstm_8 (LSTM)               (None, 1, 64)                17152     ['tf.expand_dims_4[0][0]']    \n",
            "                                                                                                  \n",
            " lstm_10 (LSTM)              (None, 1, 64)                17664     ['tf.expand_dims_5[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, None, 64)             147520    ['tf_distil_bert_model_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " attention_4 (Attention)     (None, 1, 64)                0         ['lstm_8[0][0]',              \n",
            "                                                                     'lstm_8[0][0]']              \n",
            "                                                                                                  \n",
            " attention_5 (Attention)     (None, 1, 64)                0         ['lstm_10[0][0]',             \n",
            "                                                                     'lstm_10[0][0]']             \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Gl  (None, 64)                   0         ['conv1d_2[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " lstm_9 (LSTM)               (None, 64)                   33024     ['attention_4[0][0]']         \n",
            "                                                                                                  \n",
            " lstm_11 (LSTM)              (None, 64)                   33024     ['attention_5[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 192)                  0         ['global_max_pooling1d_2[0][0]\n",
            " )                                                                  ',                            \n",
            "                                                                     'lstm_9[0][0]',              \n",
            "                                                                     'lstm_11[0][0]']             \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 64)                   12352     ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_113 (Dropout)       (None, 64)                   0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 3)                    195       ['dropout_113[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109151043 (416.38 MB)\n",
            "Trainable params: 109151043 (416.38 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_tweet_train = {k: v[:len(X_prices_train)] for k, v in tweet_encodings.items()}\n",
        "X_tweet_val = {k: v[len(X_prices_train):len(X_prices_train) + len(X_prices_val)] for k, v in tweet_encodings.items()}\n",
        "X_tweet_test = {k: v[len(X_prices_train) + len(X_prices_val):] for k, v in tweet_encodings.items()}\n",
        "\n",
        "history = model.fit(\n",
        "    [X_tweet_train['input_ids'], X_prices_train, X_tech_train], y_train,\n",
        "    epochs=15,\n",
        "    batch_size=16,\n",
        "    validation_data=([X_tweet_val['input_ids'], X_prices_val, X_tech_val], y_val)\n",
        ")\n",
        "\n",
        "loss, accuracy = model.evaluate([X_tweet_test['input_ids'], X_prices_test, X_tech_test], y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7924969-d307-48bc-a6e5-be91371f17da",
        "id": "Rk_C8M9EResz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "56/56 [==============================] - 62s 330ms/step - loss: 1.0140 - accuracy: 0.4616 - val_loss: 0.7521 - val_accuracy: 0.5000\n",
            "Epoch 2/15\n",
            "56/56 [==============================] - 13s 233ms/step - loss: 0.7890 - accuracy: 0.4616 - val_loss: 0.7224 - val_accuracy: 0.5000\n",
            "Epoch 3/15\n",
            "56/56 [==============================] - 13s 227ms/step - loss: 0.7482 - accuracy: 0.5023 - val_loss: 0.7203 - val_accuracy: 0.5270\n",
            "Epoch 4/15\n",
            "56/56 [==============================] - 13s 226ms/step - loss: 0.7308 - accuracy: 0.5034 - val_loss: 0.7230 - val_accuracy: 0.4730\n",
            "Epoch 5/15\n",
            "56/56 [==============================] - 12s 220ms/step - loss: 0.7394 - accuracy: 0.4853 - val_loss: 0.7149 - val_accuracy: 0.5034\n",
            "Epoch 6/15\n",
            "56/56 [==============================] - 12s 214ms/step - loss: 0.7197 - accuracy: 0.5181 - val_loss: 0.7139 - val_accuracy: 0.5034\n",
            "Epoch 7/15\n",
            "56/56 [==============================] - 12s 209ms/step - loss: 0.7103 - accuracy: 0.5079 - val_loss: 0.7157 - val_accuracy: 0.4730\n",
            "Epoch 8/15\n",
            "56/56 [==============================] - 12s 210ms/step - loss: 0.7073 - accuracy: 0.5135 - val_loss: 0.7141 - val_accuracy: 0.5034\n",
            "Epoch 9/15\n",
            "56/56 [==============================] - 12s 208ms/step - loss: 0.7114 - accuracy: 0.5045 - val_loss: 0.7137 - val_accuracy: 0.5068\n",
            "Epoch 10/15\n",
            "56/56 [==============================] - 12s 207ms/step - loss: 0.7140 - accuracy: 0.5090 - val_loss: 0.7145 - val_accuracy: 0.5068\n",
            "Epoch 11/15\n",
            "56/56 [==============================] - 12s 208ms/step - loss: 0.7058 - accuracy: 0.5147 - val_loss: 0.7158 - val_accuracy: 0.5068\n",
            "Epoch 12/15\n",
            "56/56 [==============================] - 12s 209ms/step - loss: 0.7067 - accuracy: 0.5158 - val_loss: 0.7133 - val_accuracy: 0.5068\n",
            "Epoch 13/15\n",
            "56/56 [==============================] - 12s 209ms/step - loss: 0.7118 - accuracy: 0.5090 - val_loss: 0.7133 - val_accuracy: 0.5068\n",
            "Epoch 14/15\n",
            "56/56 [==============================] - 12s 210ms/step - loss: 0.7071 - accuracy: 0.5327 - val_loss: 0.7111 - val_accuracy: 0.5101\n",
            "Epoch 15/15\n",
            "56/56 [==============================] - 12s 210ms/step - loss: 0.7061 - accuracy: 0.5350 - val_loss: 0.7126 - val_accuracy: 0.5068\n",
            "10/10 [==============================] - 1s 112ms/step - loss: 0.7297 - accuracy: 0.4561\n",
            "Test Loss: 0.7296596169471741, Test Accuracy: 0.4560810923576355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate([X_tweet_test['input_ids'], X_prices_test, X_tech_test], y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c88b434-4066-46c5-9950-7cd22c6b02f1",
        "id": "aeXeSHz-Res0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 1s 111ms/step - loss: 0.7297 - accuracy: 0.4561\n",
            "Test Loss: 0.7296596169471741, Test Accuracy: 0.4560810923576355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GOOG"
      ],
      "metadata": {
        "id": "iCw6utMCSmZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "goog = tweet_data_by_stock['GOOG']\n",
        "\n",
        "goog['datetime'] = pd.to_datetime(goog['post_date'], utc=True)\n",
        "\n",
        "# Convert to local time zone if needed (e.g., US Eastern Time)\n",
        "goog['datetime'] = goog['datetime'].dt.tz_convert('America/New_York')\n",
        "\n",
        "# Aggregate tweets by hour\n",
        "goog_aggregated = goog.groupby(goog['datetime'].dt.floor('H'))['body'].apply(' '.join).reset_index()\n",
        "goog_aggregated.columns = ['datetime', 'aggregated_tweet']"
      ],
      "metadata": {
        "id": "idZDBJIHSo6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices = pd.read_csv('/content/drive/MyDrive/dldata/h1_2015_GOOG_prices_tech.csv')"
      ],
      "metadata": {
        "id": "MrsU-aszSo6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices['datetime'] = pd.to_datetime(prices['TIME'], utc=True)\n",
        "prices['datetime'] = prices['datetime'].dt.tz_convert('America/New_York')"
      ],
      "metadata": {
        "id": "S-GeACn8So6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = pd.merge(prices, amzn_aggregated, left_on='datetime', right_on='datetime')"
      ],
      "metadata": {
        "id": "ziaXll-ASo6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tech = prices[['returns','SP-EMA5','OBV','MACD']]"
      ],
      "metadata": {
        "id": "CGjh5OMHSo6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = merged.dropna(subset=['returns', 'PRICE'])"
      ],
      "metadata": {
        "id": "dOmPH6SfSo6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_features = ['PRICE', 'SIZE']\n",
        "tech_features = ['returns','SP-EMA5','OBV','MACD']\n",
        "X_prices = merged[price_features].values\n",
        "X_tech = merged[tech_features].values\n",
        "\n",
        "y = merged['return_label'].shift(-1).dropna()\n",
        "X_prices = X_prices[:-1]\n",
        "X_tech = X_tech[:-1]"
      ],
      "metadata": {
        "id": "UPpYSM4FSo6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(y)\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "y_encoded = onehot_encoder.fit_transform(integer_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac66ccf-e6a4-4ab4-da0c-238c543ab0a6",
        "id": "Y9OoJetHSo6H"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_texts = merged['aggregated_tweet'].values  # Assuming 'tweets' column has the aggregated tweets\n",
        "tweet_encodings = tokenizer(list(tweet_texts[:-1]), truncation=True, padding=True, max_length=max_seq_length, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "hc1UOCyWSo6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tweet_encodings['input_ids']\n",
        "attention_mask = tweet_encodings['attention_mask']\n",
        "#token_type_ids = tweet_encodings['token_type_ids']"
      ],
      "metadata": {
        "id": "7MWRTwwBSo6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_prices_train, X_prices_test, X_tech_train, X_tech_test, y_train, y_test = train_test_split(\n",
        "    X_prices, X_tech, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_prices_train, X_prices_val, X_tech_train, X_tech_val, y_train, y_val = train_test_split(\n",
        "    X_prices_train, X_tech_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
      ],
      "metadata": {
        "id": "XfVDx1PNSo6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c66c4e-3cfa-41b6-bab5-331167dcc1eb",
        "id": "ZOhWTGxASo6I"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'cls.predictions.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertModel were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.11.output_layer_norm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "def create_model():\n",
        "    tweet_input = Input(shape=(None,), dtype=tf.int32, name='tweet_input')\n",
        "    bert_model = TFDistilBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    bert_embeddings = bert_model(tweet_input)[0]\n",
        "    cnn_layer = Conv1D(64, kernel_size=3, activation='relu')(bert_embeddings)\n",
        "    cnn_layer = GlobalMaxPooling1D()(cnn_layer)\n",
        "\n",
        "\n",
        "    price_input = Input(shape=(len(price_features),), name='price_input')\n",
        "    lstm_price = LSTM(64, return_sequences=True)(tf.expand_dims(price_input, axis=1))\n",
        "    attention_price = Attention()([lstm_price, lstm_price])\n",
        "    lstm_price_output = LSTM(64)(attention_price)\n",
        "\n",
        "    tech_input = Input(shape=(len(tech_features),), name='tech_input')\n",
        "    lstm_tech = LSTM(64, return_sequences=True)(tf.expand_dims(tech_input, axis=1))\n",
        "    attention_tech = Attention()([lstm_tech, lstm_tech])\n",
        "    lstm_tech_output = LSTM(64)(attention_tech)\n",
        "\n",
        "    concatenated = Concatenate(axis=1)([cnn_layer, lstm_price_output, lstm_tech_output])\n",
        "    dense1 = Dense(64, activation='relu')(concatenated)\n",
        "    dropout = Dropout(0.5)(dense1)\n",
        "    output = Dense(3, activation='softmax')(dropout)\n",
        "\n",
        "    model = Model(inputs=[tweet_input, price_input, tech_input], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0328c0be-eff2-4b97-edee-67ef5f271174",
        "id": "vJ3Frc9xSo6I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " price_input (InputLayer)    [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " tech_input (InputLayer)     [(None, 4)]                  0         []                            \n",
            "                                                                                                  \n",
            " tweet_input (InputLayer)    [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " tf.expand_dims_6 (TFOpLamb  (None, 1, 2)                 0         ['price_input[0][0]']         \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.expand_dims_7 (TFOpLamb  (None, 1, 4)                 0         ['tech_input[0][0]']          \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf_distil_bert_model_3 (TF  TFBaseModelOutput(last_hid   1088901   ['tweet_input[0][0]']         \n",
            " DistilBertModel)            den_state=(None, None, 768   12                                      \n",
            "                             ),                                                                   \n",
            "                              hidden_states=None, atten                                           \n",
            "                             tions=None)                                                          \n",
            "                                                                                                  \n",
            " lstm_12 (LSTM)              (None, 1, 64)                17152     ['tf.expand_dims_6[0][0]']    \n",
            "                                                                                                  \n",
            " lstm_14 (LSTM)              (None, 1, 64)                17664     ['tf.expand_dims_7[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, None, 64)             147520    ['tf_distil_bert_model_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " attention_6 (Attention)     (None, 1, 64)                0         ['lstm_12[0][0]',             \n",
            "                                                                     'lstm_12[0][0]']             \n",
            "                                                                                                  \n",
            " attention_7 (Attention)     (None, 1, 64)                0         ['lstm_14[0][0]',             \n",
            "                                                                     'lstm_14[0][0]']             \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Gl  (None, 64)                   0         ['conv1d_3[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " lstm_13 (LSTM)              (None, 64)                   33024     ['attention_6[0][0]']         \n",
            "                                                                                                  \n",
            " lstm_15 (LSTM)              (None, 64)                   33024     ['attention_7[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate  (None, 192)                  0         ['global_max_pooling1d_3[0][0]\n",
            " )                                                                  ',                            \n",
            "                                                                     'lstm_13[0][0]',             \n",
            "                                                                     'lstm_15[0][0]']             \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 64)                   12352     ['concatenate_3[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_151 (Dropout)       (None, 64)                   0         ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 3)                    195       ['dropout_151[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109151043 (416.38 MB)\n",
            "Trainable params: 109151043 (416.38 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_tweet_train = {k: v[:len(X_prices_train)] for k, v in tweet_encodings.items()}\n",
        "X_tweet_val = {k: v[len(X_prices_train):len(X_prices_train) + len(X_prices_val)] for k, v in tweet_encodings.items()}\n",
        "X_tweet_test = {k: v[len(X_prices_train) + len(X_prices_val):] for k, v in tweet_encodings.items()}\n",
        "\n",
        "history = model.fit(\n",
        "    [X_tweet_train['input_ids'], X_prices_train, X_tech_train], y_train,\n",
        "    epochs=15,\n",
        "    batch_size=16,\n",
        "    validation_data=([X_tweet_val['input_ids'], X_prices_val, X_tech_val], y_val)\n",
        ")\n",
        "\n",
        "loss, accuracy = model.evaluate([X_tweet_test['input_ids'], X_prices_test, X_tech_test], y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e8ff1ee-1994-4836-e98d-aa6b4e5a86d5",
        "id": "dYGvg4iESo6I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "49/49 [==============================] - 61s 367ms/step - loss: 0.9833 - accuracy: 0.4948 - val_loss: 0.7758 - val_accuracy: 0.4903\n",
            "Epoch 2/15\n",
            "49/49 [==============================] - 11s 232ms/step - loss: 0.7900 - accuracy: 0.5026 - val_loss: 0.7969 - val_accuracy: 0.4981\n",
            "Epoch 3/15\n",
            "49/49 [==============================] - 11s 233ms/step - loss: 0.7460 - accuracy: 0.5208 - val_loss: 0.7489 - val_accuracy: 0.4864\n",
            "Epoch 4/15\n",
            "49/49 [==============================] - 11s 223ms/step - loss: 0.7406 - accuracy: 0.4818 - val_loss: 0.7444 - val_accuracy: 0.4864\n",
            "Epoch 5/15\n",
            "49/49 [==============================] - 11s 217ms/step - loss: 0.7250 - accuracy: 0.5013 - val_loss: 0.7508 - val_accuracy: 0.4903\n",
            "Epoch 6/15\n",
            "49/49 [==============================] - 11s 217ms/step - loss: 0.7087 - accuracy: 0.5169 - val_loss: 0.7575 - val_accuracy: 0.4864\n",
            "Epoch 7/15\n",
            "49/49 [==============================] - 10s 211ms/step - loss: 0.7051 - accuracy: 0.5273 - val_loss: 0.7596 - val_accuracy: 0.4903\n",
            "Epoch 8/15\n",
            "49/49 [==============================] - 10s 211ms/step - loss: 0.7184 - accuracy: 0.4649 - val_loss: 0.7567 - val_accuracy: 0.4747\n",
            "Epoch 9/15\n",
            "49/49 [==============================] - 10s 209ms/step - loss: 0.7133 - accuracy: 0.4961 - val_loss: 0.7616 - val_accuracy: 0.4903\n",
            "Epoch 10/15\n",
            "49/49 [==============================] - 10s 209ms/step - loss: 0.7067 - accuracy: 0.5130 - val_loss: 0.7648 - val_accuracy: 0.4903\n",
            "Epoch 11/15\n",
            "49/49 [==============================] - 10s 206ms/step - loss: 0.7081 - accuracy: 0.4857 - val_loss: 0.7671 - val_accuracy: 0.4981\n",
            "Epoch 12/15\n",
            "49/49 [==============================] - 10s 209ms/step - loss: 0.7045 - accuracy: 0.4870 - val_loss: 0.7719 - val_accuracy: 0.4903\n",
            "Epoch 13/15\n",
            "49/49 [==============================] - 10s 209ms/step - loss: 0.7068 - accuracy: 0.4896 - val_loss: 0.7694 - val_accuracy: 0.4825\n",
            "Epoch 14/15\n",
            "49/49 [==============================] - 10s 206ms/step - loss: 0.7031 - accuracy: 0.5065 - val_loss: 0.7746 - val_accuracy: 0.4981\n",
            "Epoch 15/15\n",
            "49/49 [==============================] - 10s 210ms/step - loss: 0.7005 - accuracy: 0.5143 - val_loss: 0.7744 - val_accuracy: 0.4903\n",
            "9/9 [==============================] - 1s 111ms/step - loss: 0.7121 - accuracy: 0.5214\n",
            "Test Loss: 0.712134063243866, Test Accuracy: 0.5214007496833801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate([X_tweet_test['input_ids'], X_prices_test, X_tech_test], y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "509984b1-8a90-4f00-8e17-82b5c1c231c5",
        "id": "BQUYGJx6So6I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 1s 111ms/step - loss: 0.7121 - accuracy: 0.5214\n",
            "Test Loss: 0.712134063243866, Test Accuracy: 0.5214007496833801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_prices_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDPofej6UCI4",
        "outputId": "62ec6f6c-ce96-4d6c-a42c-279908ad605a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "257"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    }
  ]
}