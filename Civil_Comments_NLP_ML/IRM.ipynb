{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdhzt4qPmHxo",
        "outputId": "c255c0fd-143a-4fba-a0eb-b6620bf99f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wilds in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from wilds) (1.22.4)\n",
            "Requirement already satisfied: ogb>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from wilds) (1.3.6)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from wilds) (0.2.2)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from wilds) (1.5.3)\n",
            "Requirement already satisfied: pillow>=7.2.0 in /usr/local/lib/python3.10/dist-packages (from wilds) (8.4.0)\n",
            "Requirement already satisfied: pytz>=2020.4 in /usr/local/lib/python3.10/dist-packages (from wilds) (2022.7.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from wilds) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from wilds) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.53.0 in /usr/local/lib/python3.10/dist-packages (from wilds) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from wilds) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from wilds) (1.10.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.2.6->wilds) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.2.6->wilds) (1.26.15)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->wilds) (67.7.2)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->wilds) (0.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->wilds) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->wilds) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->wilds) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->wilds) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->wilds) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->wilds) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->wilds) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->wilds) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->wilds) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->wilds) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->wilds) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install wilds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n"
      ],
      "metadata": {
        "id": "zwAhJOdXkYSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS2YyNx5RSNn",
        "outputId": "4b9fdf0b-ed7d-4233-d52e-b5b5fbe71339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2101/2101 [00:57<00:00, 36.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100,  Train Acc: 14.6537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2102/2102 [00:33<00:00, 63.61it/s, MSE=0.412]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Test Loss: 0.0032, Test Acc: 0.8203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2101/2101 [00:57<00:00, 36.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100,  Train Acc: 20.2337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2102/2102 [00:33<00:00, 62.67it/s, MSE=0.784]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Test Loss: 0.0061, Test Acc: 0.5312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2101/2101 [00:57<00:00, 36.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100,  Train Acc: 20.5740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2102/2102 [00:35<00:00, 59.69it/s, MSE=0.375]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100, Test Loss: 0.0029, Test Acc: 0.7578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2101/2101 [00:57<00:00, 36.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100,  Train Acc: 20.3428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2102/2102 [00:36<00:00, 57.25it/s, MSE=0.555]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100, Test Loss: 0.0043, Test Acc: 0.7422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2101/2101 [00:57<00:00, 36.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100,  Train Acc: 20.3249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2102/2102 [00:34<00:00, 60.46it/s, MSE=0.674]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100, Test Loss: 0.0053, Test Acc: 0.7109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2101/2101 [00:57<00:00, 36.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100,  Train Acc: 20.6306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2102/2102 [00:37<00:00, 56.13it/s, MSE=0.465]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100, Test Loss: 0.0036, Test Acc: 0.8203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2101/2101 [00:57<00:00, 36.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100,  Train Acc: 20.3096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2102/2102 [00:33<00:00, 62.26it/s, MSE=0.436]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100, Test Loss: 0.0034, Test Acc: 0.8359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2101/2101 [00:57<00:00, 36.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100,  Train Acc: 20.5490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2102/2102 [00:35<00:00, 58.97it/s, MSE=0.126]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100, Test Loss: 0.0010, Test Acc: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2101/2101 [00:57<00:00, 36.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100,  Train Acc: 20.4111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2102/2102 [00:37<00:00, 56.62it/s, MSE=0.465]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100, Test Loss: 0.0036, Test Acc: 0.7812\n",
            "({'acc_avg': 0.5523015260696411, 'acc_y:0_male:1': 0.498511403799057, 'count_y:0_male:1': 12092.0, 'acc_y:1_male:1': 0.7852928042411804, 'count_y:1_male:1': 2203.0, 'acc_y:0_female:1': 0.5364976525306702, 'count_y:0_female:1': 14179.0, 'acc_y:1_female:1': 0.7484581470489502, 'count_y:1_female:1': 2270.0, 'acc_y:0_LGBTQ:1': 0.4261682331562042, 'count_y:0_LGBTQ:1': 3210.0, 'acc_y:1_LGBTQ:1': 0.8034539222717285, 'count_y:1_LGBTQ:1': 1216.0, 'acc_y:0_christian:1': 0.6769688725471497, 'count_y:0_christian:1': 12101.0, 'acc_y:1_christian:1': 0.6817460060119629, 'count_y:1_christian:1': 1260.0, 'acc_y:0_muslim:1': 0.4461251199245453, 'count_y:0_muslim:1': 5355.0, 'acc_y:1_muslim:1': 0.8008604645729065, 'count_y:1_muslim:1': 1627.0, 'acc_y:0_other_religions:1': 0.5664429664611816, 'count_y:0_other_religions:1': 2980.0, 'acc_y:1_other_religions:1': 0.7384615540504456, 'count_y:1_other_religions:1': 520.0, 'acc_y:0_black:1': 0.4023987948894501, 'count_y:0_black:1': 3335.0, 'acc_y:1_black:1': 0.8132725954055786, 'count_y:1_black:1': 1537.0, 'acc_y:0_white:1': 0.39087891578674316, 'count_y:0_white:1': 5723.0, 'acc_y:1_white:1': 0.8143365979194641, 'count_y:1_white:1': 2246.0, 'acc_wg': 0.39087891578674316}, 'Average acc: 0.552\\n  male                   acc on non_toxic: 0.499 (n =  12092)    acc on toxic: 0.785 (n =   2203) \\n  female                 acc on non_toxic: 0.536 (n =  14179)    acc on toxic: 0.748 (n =   2270) \\n  LGBTQ                  acc on non_toxic: 0.426 (n =   3210)    acc on toxic: 0.803 (n =   1216) \\n  christian              acc on non_toxic: 0.677 (n =  12101)    acc on toxic: 0.682 (n =   1260) \\n  muslim                 acc on non_toxic: 0.446 (n =   5355)    acc on toxic: 0.801 (n =   1627) \\n  other_religions        acc on non_toxic: 0.566 (n =   2980)    acc on toxic: 0.738 (n =    520) \\n  black                  acc on non_toxic: 0.402 (n =   3335)    acc on toxic: 0.813 (n =   1537) \\n  white                  acc on non_toxic: 0.391 (n =   5723)    acc on toxic: 0.814 (n =   2246) \\nWorst-group acc: 0.391\\n')\n"
          ]
        }
      ],
      "source": [
        "from torchtext.vocab import GloVe\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import GloVe\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from wilds import get_dataset\n",
        "from wilds.common.data_loaders import get_train_loader\n",
        "import torchvision.transforms as transforms\n",
        "from wilds.common.grouper import CombinatorialGrouper\n",
        "from wilds.common.utils import split_into_groups\n",
        "from torch.autograd import grad\n",
        "from wilds.common.data_loaders import get_eval_loader\n",
        "\n",
        "\n",
        "class ToxicClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, embeddings_vectors, hidden_dim = 32, output_dim = 1):\n",
        "        super(ToxicClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embeddings_vectors, freeze=True)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.output = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        _, embedded = self.rnn(embedded)\n",
        "        return self.output(embedded[-1])\n",
        "\n",
        "def tokenize(text, max_length = 100):\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"([.!?,'*])\", r\"\", text)\n",
        "    text = re.sub(r\"([-])\", r\" \", text)\n",
        "    tokens = tokenizer(text)\n",
        "    if len(tokens) < max_length:\n",
        "      tokens.extend(['<PAD>']*(max_length - len(tokens)))\n",
        "    tokens = tokens[:max_length]\n",
        "    tokens = [glove.stoi.get(token, len(glove.stoi) - 1) for token in tokens]\n",
        "    tokens = np.array(tokens, dtype=np.int64)\n",
        "    return tokens\n",
        "\n",
        "def compute_irm_penalty(losses, dummy):\n",
        "  g1 = grad(losses[0::2].mean(), dummy, create_graph=True)[0]\n",
        "  g2 = grad(losses[1::2].mean(), dummy, create_graph=True)[0]\n",
        "  return (g1 * g2).sum()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 32\n",
        "token_dim = 50\n",
        "length = 100\n",
        "\n",
        "glove = GloVe(name='6B', dim=token_dim)\n",
        "padding_vector = torch.zeros(token_dim)\n",
        "padding_token = '<PAD>'\n",
        "glove.itos.append(padding_token)  \n",
        "glove.stoi[padding_token] = len(glove.itos) - 1 \n",
        "glove.vectors = torch.cat((glove.vectors, padding_vector.unsqueeze(0)), dim=0) \n",
        "\n",
        "dataset = get_dataset(dataset=\"civilcomments\", download=True)\n",
        "train_data = dataset.get_subset(\n",
        "    \"train\")\n",
        "train_loader = get_train_loader(\"standard\", train_data, batch_size=128)\n",
        "test_data = dataset.get_subset(\n",
        "    \"val\")\n",
        "test_loader = get_train_loader(\"standard\", train_data, batch_size=128)\n",
        "\n",
        "identities = CombinatorialGrouper(dataset, [\n",
        "            'male',\n",
        "            'female',\n",
        "            'LGBTQ',\n",
        "            'christian',\n",
        "            'muslim',\n",
        "            'other_religions',\n",
        "            'black',\n",
        "            'white'\n",
        "        ])\n",
        "train_loader = get_train_loader(\n",
        "    \"group\", train_data, grouper=identities, n_groups_per_batch=4, batch_size=128\n",
        ")\n",
        "\n",
        "\n",
        "model = ToxicClassifier(len(glove), token_dim, glove.vectors.to(device))\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience = 10)\n",
        "\n",
        "negative_samples = (train_data.y_array == 0).sum()\n",
        "positive_samples = (train_data.y_array == 1).sum()\n",
        "pos_weight = negative_samples / positive_samples\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(0, num_epochs):\n",
        "  train_loss = 0.0\n",
        "  train_correct = 0\n",
        "  train_total = 0\n",
        "  model.train()\n",
        "  dummy_w = torch.nn.Parameter(torch.Tensor([1.0])).to(device)\n",
        "  penalty_multiplier = epoch ** 1.6\n",
        "  for cur_train in tqdm(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      error = 0\n",
        "      penalty = 0\n",
        "      input, label, metadata = cur_train\n",
        "      idx, groups_idx, _ = split_into_groups(identities.metadata_to_group(metadata))\n",
        "      input = tuple(map(tokenize, input))\n",
        "      input = torch.Tensor(input).long()\n",
        "      for i in groups_idx:\n",
        "        cur_in = torch.index_select(input, 0, i).to(device)\n",
        "        cur_label = torch.index_select(label, 0 , i).to(device)\n",
        "        output = model(cur_in)\n",
        "        predicted_labels = (torch.sigmoid(output) >= 0.5).float()\n",
        "        loss_erm = F.binary_cross_entropy_with_logits((output * dummy_w).reshape(-1), cur_label.float(), reduction='none', pos_weight= pos_weight)\n",
        "        penalty += compute_irm_penalty(loss_erm, dummy_w)\n",
        "        error += loss_erm.mean()\n",
        "        train_correct += (predicted_labels == cur_label).sum().item()\n",
        "        train_total += len(cur_label)\n",
        "      (error + penalty_multiplier * penalty).backward()\n",
        "      optimizer.step()\n",
        "  scheduler.step(train_loss)\n",
        "  train_acc = train_correct / train_total\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs},  Train Acc: {train_acc:.4f}\")  \n",
        "\n",
        "  test_loss = 0.0\n",
        "  test_correct = 0\n",
        "  test_total = 0\n",
        "  pbar = tqdm(test_loader)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for i, data in enumerate(pbar, 0):\n",
        "          input, label, groupings = cur_train\n",
        "          input = tuple(map(tokenize, input))\n",
        "          input = torch.Tensor(input).long().to(device)\n",
        "          label = label.to(device)\n",
        "          output = model(input).reshape(-1)\n",
        "          loss = criterion(output, label.float())\n",
        "          predicted_labels = (torch.sigmoid(output) >= 0.5).float()\n",
        "          test_total += len(label)\n",
        "          test_correct += (predicted_labels == label).sum().item()\n",
        "          test_loss += loss\n",
        "          pbar.set_postfix(MSE=loss.item())\n",
        "      \n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_acc = test_correct / test_total\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "test_data = dataset.get_subset(\n",
        "    \"test\",\n",
        ")\n",
        "# Prepare the data loader\n",
        "test_loader = get_eval_loader(\"standard\", test_data, batch_size=32)\n",
        "trues = []\n",
        "preds = []\n",
        "metadatas = []\n",
        "for input, true, metadata in test_loader:\n",
        "    with torch.no_grad():\n",
        "      input = tuple(map(tokenize, input))\n",
        "      input = torch.Tensor(input).long().to(device)\n",
        "      output = model(input)\n",
        "      output = (torch.sigmoid(output) >= 0.5).long().reshape(-1)\n",
        "      trues.append(true.to('cpu'))\n",
        "      preds.append(output.to('cpu'))\n",
        "      metadatas.append(metadata.to('cpu'))\n",
        "all_preds = torch.cat(preds, dim = 0)\n",
        "all_trues = torch.cat(trues, dim = 0)\n",
        "all_metas = torch.cat(metadatas, dim = 0)\n",
        "print(dataset.eval(all_preds, all_trues, all_metas))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}